{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from paths_bra import *\n",
    "\n",
    "from dask.diagnostics import ProgressBar\n",
    "ProgressBar().register()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare turbine location data\n"
     ]
    }
   ],
   "source": [
    "# MERRA-2 and ERA5 only unique interpolated locations\n",
    "print('prepare turbine location data')\n",
    "# open turbine files\n",
    "wt_mer = pd.read_csv(bra_path + '/turbine_data_mer.csv', index_col=0)\n",
    "wt_era = pd.read_csv(bra_path + '/turbine_data_era.csv', index_col=0)\n",
    "\n",
    "# open wind files\n",
    "wind_mer = xr.open_mfdataset(mer_path + \"/eff_ws/merra2_wind_BRA_*.nc\", chunks = {'time': 38})\n",
    "alpha_mer = xr.open_mfdataset(mer_path + \"/eff_ws/merra2_alpha_BRA_*.nc\", chunks = {'time': 38})\n",
    "wind_era = xr.open_mfdataset(era_path + \"/eff_ws/era5_wind_BRA_*.nc\", chunks = {'time': 38})\n",
    "alpha_era = xr.open_mfdataset(era_path + \"/eff_ws/era5_alpha_BRA_*.nc\", chunks = {'time': 38})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n"
     ]
    }
   ],
   "source": [
    "# Create dataframe with sequence the size of MERRA-2 grid to find out which turbines interpolate to the same point\n",
    "in_seq_mer = xr.Dataset({'x':(['lat','lon'],\n",
    "                              np.array(range(wind_mer.wh50.isel(time=0).values.size)).reshape(wind_mer.wh50.isel(time=0).values.shape))},\n",
    "                         coords = {'lat':wind_mer.lat.values,\n",
    "                                   'lon':wind_mer.lon.values})\n",
    "in_seq_era = xr.Dataset({'x':(['lat','lon'],\n",
    "                              np.array(range(wind_era.wh100.isel(time=0).values.size)).reshape(wind_era.wh100.isel(time=0).values.shape))},\n",
    "                         coords = {'lat':wind_era.latitude.values,\n",
    "                                   'lon':wind_era.longitude.values})\n",
    "\n",
    "# interpolate to reanalysis grid points\n",
    "ip_mer = in_seq_mer.interp(coords={\"lon\":xr.DataArray(wt_mer.lon,dims='location'),\n",
    "                                   \"lat\":xr.DataArray(wt_mer.lat,dims='location')},method=\"nearest\").to_dataframe()\n",
    "ip_era = in_seq_era.interp(coords={\"lon\":xr.DataArray(wt_era.lon,dims='location'),\n",
    "                                   \"lat\":xr.DataArray(wt_era.lat,dims='location')},method=\"nearest\").to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find unique locations\n",
    "uniques_mer = ip_mer.groupby(ip_mer.x).min()\n",
    "uniques_era = ip_era.groupby(ip_era.x).min()\n",
    "\n",
    "# add ids to unique correlation locations\n",
    "uniques_era['cor_id'] = range(len(uniques_era.index))\n",
    "uniques_mer['cor_id'] = range(len(uniques_mer.index))\n",
    "\n",
    "# add correlation ids to wind turbine data\n",
    "wt_mer['cor_id'] = ip_mer.x.map(uniques_mer.cor_id)\n",
    "wt_era['cor_id'] = ip_era.x.map(uniques_era.cor_id)\n",
    "\n",
    "# interpolate wind to unique locations and extrapolate to 90 m hubheight, which is mean hubheight in BRA\n",
    "windi_mer = wind_mer.interp(coords={\"lon\":xr.DataArray(uniques_mer.lon,dims='location'),\n",
    "                                    \"lat\":xr.DataArray(uniques_mer.lat,dims='location')},method=\"nearest\")\n",
    "windi_era = wind_era.interp(coords={\"longitude\":xr.DataArray(uniques_era.lon,dims='location'),\n",
    "                                    \"latitude\":xr.DataArray(uniques_era.lat,dims='location')},method=\"nearest\")\n",
    "                                    \n",
    "alphai_mer = alpha_mer.interp(coords={\"lon\":xr.DataArray(uniques_mer.lon,dims='location'),\n",
    "                                      \"lat\":xr.DataArray(uniques_mer.lat,dims='location')},method=\"nearest\")\n",
    "alphai_era = alpha_era.interp(coords={\"longitude\":xr.DataArray(uniques_era.lon,dims='location'),\n",
    "                                      \"latitude\":xr.DataArray(uniques_era.lat,dims='location')},method=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate wind speeds at 90 m height\n",
    "windhh_mer = (windi_mer.wh50 * (90/50)**alphai_mer.alpha)\n",
    "windhh_era = (windi_era.wh100 * (90/100)**alphai_era.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculate cross-correlations\n"
     ]
    }
   ],
   "source": [
    "# calculate cross-correlations\n",
    "print('calculate cross-correlations')\n",
    "if not os.path.isfile(results_path + '/corr_mer.nc'):\n",
    "    c_mer = np.corrcoef(windhh_mer,rowvar=False)\n",
    "    xr.DataArray(c_mer,\n",
    "                 dims = ['id1','id2'],\n",
    "                 coords = {'id1':range(c_mer.shape[0]),\n",
    "                           'id2':range(c_mer.shape[0])}).to_netcdf(results_path + '/corr_mer.nc')\n",
    "    del(c_mer)\n",
    "if not os.path.isfile(results_path + '/corr_era.nc'):\n",
    "    c_era = np.corrcoef(windhh_era,rowvar=False)\n",
    "    xr.DataArray(c_era,\n",
    "                 dims = ['id1','id2'],\n",
    "                 coords = {'id1':range(c_era.shape[0]),\n",
    "                           'id2':range(c_era.shape[0])}).to_netcdf(results_path + '/corr_era.nc')\n",
    "    del(c_era)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load correlations\n",
    "c_mer = xr.open_dataarray(results_path + '/corr_mer.nc').values\n",
    "c_era = xr.open_dataarray(results_path + '/corr_era.nc').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANL = pd.read_csv(bra_path + '/turbine_data.csv', index_col = 0)\n",
    "lbl = pd.read_csv(bra_path+ '/labels_turbine_data_gwa3.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# usinas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some locations have more than one park, get shares of parks\n",
    "sharesMER = ANL.cap.groupby([lbl.lbl_mer.values,ANL.name.values]).sum()/ANL.cap.groupby([lbl.lbl_mer.values,ANL.name.values]).sum().index.get_level_values(0).map(ANL.cap.groupby(lbl.lbl_mer.values).sum())\n",
    "sharesERA = ANL.cap.groupby([lbl.lbl_era.values,ANL.name.values]).sum()/ANL.cap.groupby([lbl.lbl_era.values,ANL.name.values]).sum().index.get_level_values(0).map(ANL.cap.groupby(lbl.lbl_era.values).sum())\n",
    "sharesMERg = ANL.cap.groupby([lbl.lbl_mer_gwa.values,ANL.name.values]).sum()/ANL.cap.groupby([lbl.lbl_mer_gwa.values,ANL.name.values]).sum().index.get_level_values(0).map(ANL.cap.groupby(lbl.lbl_mer_gwa.values).sum())\n",
    "sharesERAg = ANL.cap.groupby([lbl.lbl_era_gwa.values,ANL.name.values]).sum()/ANL.cap.groupby([lbl.lbl_era_gwa.values,ANL.name.values]).sum().index.get_level_values(0).map(ANL.cap.groupby(lbl.lbl_era_gwa.values).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add correlation ids to shares\n",
    "sharesMER = pd.DataFrame({'share':sharesMER,\n",
    "                          'cor_id':sharesMER.index.get_level_values(0).map(pd.Series(wt_mer.cor_id.values,index=lbl.lbl_mer.unique()))})\n",
    "sharesERA = pd.DataFrame({'share':sharesERA,\n",
    "                          'cor_id':sharesERA.index.get_level_values(0).map(pd.Series(wt_era.cor_id.values,index=lbl.lbl_era.unique()))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group ids by park\n",
    "cidMER = sharesMER.groupby(sharesMER.index.get_level_values(1)).cor_id.unique()\n",
    "cidERA = sharesERA.groupby(sharesERA.index.get_level_values(1)).cor_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getmeancorr(ids,cors):\n",
    "    idu = np.unique(ids)\n",
    "    if(len(idu))>1:\n",
    "        mc = np.array([list(cors[idu[i],\n",
    "                       np.delete(idu,i,0)]) for i in range(len(idu))]).mean()\n",
    "        #return(cors[np.unique(ids)[0],np.unique(ids)[1:]].mean())\n",
    "        return(mc)\n",
    "    else:\n",
    "        return(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmc_mer(ids):\n",
    "    return(getmeancorr(ids,c_mer))\n",
    "def gmc_era(ids):\n",
    "    return(getmeancorr(ids,c_era))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mean correlations per park\n",
    "cm_USI_mer = cidMER.apply(gmc_mer)\n",
    "cm_USI_era = cidERA.apply(gmc_era)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load matching parks\n",
    "mpM = pd.read_pickle(bra_path + '/matches2.pkl')\n",
    "mpH = pd.read_pickle(bra_path + '/matches2H.pkl')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cm_EST_mer = wt_mer.groupby('state').cor_id.unique().apply(gmc_mer)\n",
    "cm_EST_era = wt_era.groupby('state').cor_id.unique().apply(gmc_era)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get correlation ids per park found in hourly and monthly data\n",
    "cid_usi_merM = pd.Series(mpM.ANL_name[mpM.score==100].map(cidMER).values,index = mpM.ANL_name[mpM.score==100])\n",
    "cid_usi_eraM = pd.Series(mpM.ANL_name[mpM.score==100].map(cidERA).values,index = mpM.ANL_name[mpM.score==100])\n",
    "cid_usi_merH = pd.Series(mpH.ANL_name[mpH.score==100].map(cidMER).values,index = mpH.ANL_name[mpH.score==100])\n",
    "cid_usi_eraH = pd.Series(mpH.ANL_name[mpH.score==100].map(cidERA).values,index = mpH.ANL_name[mpH.score==100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "usiest = ANL.groupby('name').state.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_EST_merM = cid_usi_merM.groupby(cid_usi_merM.index.map(usiest)).apply(np.concatenate).apply(np.unique).apply(gmc_mer)\n",
    "cm_EST_eraM = cid_usi_eraM.groupby(cid_usi_eraM.index.map(usiest)).apply(np.concatenate).apply(np.unique).apply(gmc_era)\n",
    "cm_EST_merH = cid_usi_merH.groupby(cid_usi_merH.index.map(usiest)).apply(np.concatenate).apply(np.unique).apply(gmc_mer)\n",
    "cm_EST_eraH = cid_usi_eraH.groupby(cid_usi_eraH.index.map(usiest)).apply(np.concatenate).apply(np.unique).apply(gmc_era)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# subsystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "subH = pd.Series(['NE']*3,index=['BA','CE','RN'])\n",
    "subM = pd.Series(['NE']*8+['S']*3,index=['BA','CE','MA','PB','PE','PI','RN','SE','PR','RS','SC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_SUB_merM = cid_usi_merM.groupby(cid_usi_merM.index.map(usiest).map(subM)).apply(np.concatenate).apply(np.unique).apply(gmc_mer)\n",
    "cm_SUB_eraM = cid_usi_eraM.groupby(cid_usi_eraM.index.map(usiest).map(subM)).apply(np.concatenate).apply(np.unique).apply(gmc_era)\n",
    "cm_SUB_merH = cid_usi_merH.groupby(cid_usi_merH.index.map(usiest).map(subH)).apply(np.concatenate).apply(np.unique).apply(gmc_mer)\n",
    "cm_SUB_eraH = cid_usi_eraH.groupby(cid_usi_eraH.index.map(usiest).map(subH)).apply(np.concatenate).apply(np.unique).apply(gmc_era)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brazil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['MERRA2', 'MERRA2', 'MERRA2', 'MERRA2', 'MERRA2', 'MERRA2',\n",
       "       'MERRA2', 'MERRA2', 'MERRA2', 'MERRA2', 'MERRA2', 'MERRA2', 'ERA5',\n",
       "       'ERA5', 'ERA5', 'ERA5', 'ERA5', 'ERA5', 'ERA5', 'ERA5', 'ERA5',\n",
       "       'ERA5', 'ERA5', 'ERA5'], dtype='<U6')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.repeat(['MERRA2','ERA5'],len(cm_EST_merM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_BRA_mer = gmc_mer(range(len(uniques_mer)))\n",
    "cm_BRA_era = gmc_era(range(len(uniques_era)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge all correlations\n"
     ]
    }
   ],
   "source": [
    "# merge correlations\n",
    "print('merge all correlations')\n",
    "mean_cors = pd.DataFrame({'scale': ['country']*6 + \n",
    "                                   ['state']*2*(2*len(cm_EST_merH)+len(cm_EST_merM)) + \n",
    "                                    ['subsystem']*2*(2*len(cm_SUB_merH)+len(cm_SUB_merM)) + \n",
    "                                    ['park']*6*len(cm_USI_mer),\n",
    "                          'region': ['BRA',]*6 + \n",
    "                                    cm_EST_merH.index.to_list()*4 + cm_EST_merM.index.to_list()*2 +\n",
    "                                    ['NE']*4 + ['NE','S']*2 + \n",
    "                                    cm_USI_mer.index.to_list()*6,\n",
    "                          'dataset': np.repeat(['MERRA2','ERA5'],3).tolist() + \n",
    "                                     np.repeat(['MERRA2','ERA5'],len(cm_EST_merH)).tolist()*2 + np.repeat(['MERRA2','ERA5'],len(cm_EST_merM)).tolist()+ \n",
    "                                     np.repeat(['MERRA2','ERA5'],len(cm_SUB_merH)).tolist()*2 + np.repeat(['MERRA2','ERA5'],len(cm_SUB_merM)).tolist() +\n",
    "                                     np.repeat(['MERRA2','ERA5'],len(cm_USI_mer)).tolist()*3,\n",
    "                          'temp':['m','d','h']*2+\n",
    "                                 np.repeat(['h','d'],2*len(cm_EST_merH)).tolist()+['m']*2*len(cm_EST_merM)+\n",
    "                                 np.repeat(['h','d'],2*len(cm_SUB_merH)).tolist()+['m']*2*len(cm_SUB_merM)+\n",
    "                                  np.repeat(['m','d','h'],2*len(cm_USI_mer)).tolist(),\n",
    "                          'cor':np.repeat([cm_BRA_mer,cm_BRA_era],3).tolist() +\n",
    "                                (cm_EST_merH.values.tolist() + cm_EST_eraH.values.tolist())*2 + cm_EST_merM.values.tolist() + cm_EST_eraM.values.tolist() +\n",
    "                                 (cm_SUB_merH.values.tolist() + cm_SUB_eraH.values.tolist())*2 + cm_SUB_merM.values.tolist() + cm_SUB_eraM.values.tolist() +\n",
    "                                 (cm_USI_mer.values.tolist() + cm_USI_era.values.tolist())*3})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# merge correlations\n",
    "print('merge all correlations')\n",
    "mean_cors = pd.DataFrame({'scale': ['BRA']*2 + ['EST']*2*(len(cm_EST_merH)+len(cm_EST_merM)) + \n",
    "                                    ['SUB']*2*(len(cm_SUB_merH)+len(cm_SUB_merM)) + \n",
    "                                    ['USI']*2*len(cm_USI_mer),\n",
    "                          'region': ['BRA',]*2 + cm_EST_merH.index.to_list()*2 + cm_EST_merM.index.to_list()*2 +\n",
    "                                      ['NE']*2 + ['NE','S']*2 + cm_USI_mer.index.to_list()*2,\n",
    "                          'dataset': ['MERRA2','ERA5'] + np.repeat(['MERRA2','ERA5'],len(cm_EST_merH)).tolist() +\n",
    "                                      np.repeat(['MERRA2','ERA5'],len(cm_EST_merM)).tolist()+ \n",
    "                                     np.repeat(['MERRA2','ERA5'],len(cm_SUB_merH)).tolist() +\n",
    "                                      np.repeat(['MERRA2','ERA5'],len(cm_SUB_merM)).tolist() +\n",
    "                                     np.repeat(['MERRA2','ERA5'],len(cm_USI_mer)).tolist(),\n",
    "                          'temp':['m']*2+['h']*2*len(cm_EST_merH)+['m']*2*len(cm_EST_merM)+\n",
    "                                  ['h']*2*len(cm_SUB_merH)+['m']*2*len(cm_SUB_merM)+\n",
    "                                  ['m']*2*len(cm_USI_mer),\n",
    "                          'cor':[cm_BRA_mer,cm_BRA_era] + cm_EST_merH.values.tolist() + cm_EST_eraH.values.tolist() +\n",
    "                                     cm_EST_merM.values.tolist() + cm_EST_eraM.values.tolist() +\n",
    "                                 cm_SUB_merH.values.tolist() + cm_SUB_eraH.values.tolist() +\n",
    "                                     cm_SUB_merM.values.tolist() + cm_SUB_eraM.values.tolist() +\n",
    "                                 cm_USI_mer.values.tolist() + cm_USI_era.values.tolist()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save wind speed correlations\n"
     ]
    }
   ],
   "source": [
    "# save wind speed correlations\n",
    "print('save wind speed correlations')\n",
    "mean_cors.to_csv(results_path + '/correlations_wind.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-py37]",
   "language": "python",
   "name": "conda-env-.conda-py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
