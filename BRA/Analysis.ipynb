{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from dateutil.relativedelta import *\n",
    "from fuzzywuzzy import fuzz\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_1samp\n",
    "import sys\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "from paths_bra import *\n",
    "\n",
    "sys.path.append('./..')\n",
    "from refuelplot import *\n",
    "setup()\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_path = bra_path + '/generation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load generation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load usinas hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "if gen_path + '/hourly/usinas.pkl' not in glob.glob(gen_path + '/hourly/*.pkl'):\n",
    "    USIh = pd.read_csv(gen_path + '/hourly/Comparativo_Geração_de_Energia_Semana_data_usinas.csv',\n",
    "                       sep = ';', index_col = 0, parse_dates = True, dayfirst = True).iloc[1:,[6,8]].sort_index()\n",
    "    # remove missing values\n",
    "    USIh = USIh.loc[USIh.index.notnull()].dropna()\n",
    "    USIh.columns = ['usina','prod_GWh']\n",
    "\n",
    "    # in RIO DO FOGO there is one duplicate hour after one missing hour -> change timestamps of those hours\n",
    "    idxUSIh = USIh.index.values\n",
    "    midxUSIh = USIh.reset_index().set_index(['usina','Data Escala de Tempo 1 GE Comp 3']).index\n",
    "    idxUSIh[midxUSIh.duplicated(keep='last')]  = idxUSIh[midxUSIh.duplicated(keep='first')] - np.timedelta64(1,'h')\n",
    "    USIh.index = pd.DatetimeIndex(idxUSIh)\n",
    "\n",
    "    USIhs = USIh.reset_index().set_index(['usina','index']).unstack(level=0).prod_GWh\n",
    "    USIhs.to_csv(gen_path + '/hourly/usinas.csv')\n",
    "    USIhs.to_pickle(gen_path + '/hourly/usinas.pkl')\n",
    "wpUSIhs = pd.read_pickle(gen_path + '/hourly/usinas.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### load and match aneel and ons windparks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_cap_df(cap,comdate):\n",
    "    com = pd.DataFrame({'capacity': cap}).groupby(comdate).sum()\n",
    "    cap_cum = com.capacity.cumsum()\n",
    "    # if only years given for commissioning dates -> gradual capacity increase over year, full capacity at end of year\n",
    "    if type(cap_cum.index.values[0]) == np.int64:\n",
    "        cap_cum.index = [np.datetime64(str(int(year))+\"-12-31 23:00:00\") for year in cap_cum.index.values]\n",
    "        # create yearly dates at yearends\n",
    "        drcc = pd.date_range(np.datetime64('2005-12-31 23:00:00'),\n",
    "                             np.datetime64('2019-12-31 23:00:00'),freq= 'y')\n",
    "        cap_cum = pd.Series(drcc.map(cap_cum),index = drcc)\n",
    "        # if first year emtpy: either year before or 0 if nothing before\n",
    "        if(sum(com.index<2000) > 0):\n",
    "            cap_cum[0] = com.cumsum()[com.index<2000].max()\n",
    "        else:\n",
    "            cap_cum[0] = 0\n",
    "        # if missing years -> put capacity of year before\n",
    "        cap_cum = cap_cum.ffill()\n",
    "    dr = pd.date_range('1/1/2006','31/12/2019 23:00:00',freq = 'h')\n",
    "    cap_ts = pd.Series(dr.map(cap_cum),index = dr)\n",
    "    cap_ts[0] = cap_cum[cap_cum.index<=pd.Timestamp('2006-01-01')].max()\n",
    "    if type(comdate[0]) == np.int64:\n",
    "        return(cap_ts.interpolate(method='linear'))\n",
    "    else:\n",
    "        return(cap_ts.fillna(method='ffill'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0,
     9,
     18,
     31,
     52,
     72,
     107
    ],
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def matchWords(word, statements):\n",
    "    # function to match a word to different statements\n",
    "    # output: ratio of matching (0-100) for all provided statements\n",
    "    results = []\n",
    "    for s in statements:\n",
    "        r = fuzz.ratio(word, s)\n",
    "        results.append(r)\n",
    "    return results\n",
    "\n",
    "def match_string(string, array):\n",
    "    # function for matching casefolded strings\n",
    "    Slc = string.strip().casefold()\n",
    "    Alc = [arr.casefold() for arr in array.str.strip().unique()]\n",
    "    scores = matchWords(Slc, Alc)\n",
    "    mscore = max(scores)\n",
    "    strarr = array.unique()[np.where(np.array(scores)==mscore)][0]\n",
    "    return(string,strarr,mscore)\n",
    "\n",
    "def match_anl(string):\n",
    "    # function to match ONS to ANL windparks\n",
    "    return(match_string(string,ANL2.name))\n",
    "\n",
    "# load ANEEL and ONS windparks\n",
    "ONS = pd.read_csv(bra_path + '/ONS_windparks.csv', index_col = 0)\n",
    "# remove those with CONJUNTO EOLICO - they're there twice and capacities don't match with ANEEL data\n",
    "ONS = ONS[~ONS.usina.str.contains('CONJUNTO EOLICO')]\n",
    "# remove some other duplicate windparks\n",
    "ONS = ONS[[d not in [' CANOA QUEBRADA (E-RV-ACEP)',' PV DO NORDESTE',' SM (SANTA MARIA)',' SÃO BENTO NORTE II'] for d in ONS.usina]]\n",
    "ANL = pd.read_csv(bra_path + '/turbine_data.csv', index_col = 0)\n",
    "\n",
    "# characters and strings to replace for better matching\n",
    "letters = {'Ãµ':'õ',\n",
    "           'ó':'o',\n",
    "           'ã':'a',\n",
    "           'á':'a',\n",
    "           'â':'a',\n",
    "           'é':'e',\n",
    "           'Ã':'A',\n",
    "           'Á':'A',\n",
    "           'Â':'A',\n",
    "           'Ó':'O',\n",
    "           'É':'E',\n",
    "           'ú':'u',\n",
    "           'ô':'o',\n",
    "           'Ô':'O',\n",
    "           'ú':'u',\n",
    "           'Ú':'U',\n",
    "           'ç':'c',\n",
    "           'Ç':'C',\n",
    "           'í':'i',\n",
    "           'Í':'I',\n",
    "           'Ê':'E'}\n",
    "remove = {' 2LER':'',\n",
    "          ' 2LFA':'',\n",
    "          ' LFA':'',\n",
    "          'EOL ':'',\n",
    "          ' 3LER':'',\n",
    "          'Usina Eolica ':'',\n",
    "          'Eólica ':'',\n",
    "          ' ENERGIAS RENOVAVEIS':'',\n",
    "#          ' CONJUNTO EOLICO':'',\n",
    "          '\\(E-BV-ACEP\\)':'',\n",
    "          '\\(E-RV-ACEP\\)':'',\n",
    "          '\\(BELA BISTA\\)':'',\n",
    "          '\\(ENERGEN\\)':'',\n",
    "          '\\(Antiga Ventos Maranhenses 05\\)':'',\n",
    "          'PARQUE EOLICO ':'',\n",
    "          ' - N HORIZ':'',\n",
    "          'ENERGETICA S/A':'',\n",
    "          '\\(ILHEUS\\)':'',\n",
    "          ' EOLOS':'',\n",
    "          'S\\.A\\.':''}\n",
    "replace = {'LAG DO':'LAGOA DO',\n",
    "           'VENTOS S VICENTE':'VENTOS DE SAO VICENTE',\n",
    "           'SERRA BABILONIA':'SERRA DA BABILONIA',\n",
    "           'CORREDOR SENANDES':'CORREDOR DO SENANDES',\n",
    "           'SAO BENTO NORTE':'SAO BENTO DO NORTE',\n",
    "           'GAMELEIRAS':'GAMELERIAS',\n",
    "           'Lagoinha':'Lagoinh',\n",
    "           'PAPAGAIOS':'PAPAGAIO',\n",
    "           'VENTOS DE SAO ABRAAO':'VENTOS DO SANTO ABRAAO',\n",
    "           'VENTOS DO SAO MARIO':'VENTOS DE SAO MARIO',\n",
    "           'DAGUA':'D AGUA',\n",
    "           'B VEN':'BONS VENTOS',\n",
    "           'NOVA BURITI':'BURITI',\n",
    "           'NOVA CAJUCOCO':'CAJUCOCO',\n",
    "           'PALMAS':'DE PALMAS',\n",
    "           'DE PALMARES':'PALMARES',\n",
    "           'PV DO NORDESTE':'VENTOS DO NORDESTE',\n",
    "           'Aura Lagoa do Barro':'Lagoa do Barro',\n",
    "           'AURA LAGOA DO BARRO':'LAGOA DO BARRO',\n",
    "           'LAGOA BARRO':'LAGOA DO BARRO',\n",
    "           'GRAVATA':'GRAVATA FRUITRADE',\n",
    "           'FAZENDA DO ROSARIO':'FAZENDA ROSARIO',\n",
    "           'Parque Eolico do Horizonte':'Ventos de Horizonte',\n",
    "           'S BENTO':'SAO BENTO',\n",
    "           'SANTO ANTONIO (BTG PACTUAL)':'SANTO ANTONIO DE PADUA',\n",
    "           'SM \\(SANTA MARIA\\)':'SANTA MARIA',\n",
    "           'SAO JORGE CE':'SAO JORGE',\n",
    "           'VENT DA ST ESPERANCA':'VENTOS DA SANTA ESPERANCA',\n",
    "           'VENTOS DA STA DULCE':'VENTOS DA SANTA DULCE',\n",
    "           'ESPERANCA NORDESTE':'ESPERANCA DO NORDESTE',\n",
    "           'Eolica Delta':'Delta',\n",
    "           'Eolica Serra das Vacas':'Serra das Vacas',\n",
    "           'Ventos de Santo Augusto':'Santo Augusto',\n",
    "           'Ventos do Sao Gabriel':'Sao Gabriel',\n",
    "           'GE Maria Helena':'Maria Helena'}\n",
    "numbers = {'10':'X',\n",
    "           '11':'XI',\n",
    "           '12':'XII',\n",
    "           '13':'XIII',\n",
    "           '14':'XIV',\n",
    "           '15':'XV',\n",
    "           '17':'XVII',\n",
    "           '19':'XIX',\n",
    "           '21':'XXI',\n",
    "           '23':'XXIII',\n",
    "           '24':'XXIV',\n",
    "           '25':'XXV',\n",
    "           '26':'XXVI',\n",
    "           '27':'XXVII',\n",
    "           '28':'XXVIII',\n",
    "           '29':'XXIX',\n",
    "           '31':'XXXI',\n",
    "           '34':'XXXIV',\n",
    "           '35':'XXXV',\n",
    "           '36':'XXXVI',\n",
    "           '01':'I',\n",
    "           '02':'II',\n",
    "           '03':'III',\n",
    "           '04':'IV',\n",
    "           '05':'V',\n",
    "           '06':'VI',\n",
    "           '07':'VII',\n",
    "           '08':'VIII',\n",
    "           '09':'IX',\n",
    "           '1':'I',\n",
    "           '2':'II',\n",
    "           '3':'III',\n",
    "           '4':'IV',\n",
    "           '5':'V',\n",
    "           '6':'VI',\n",
    "           '7':'VII',\n",
    "           '8':'VIII',\n",
    "           '9':'IX'}\n",
    "\n",
    "# replace characters\n",
    "ONS2 = ONS.copy(deep=True)\n",
    "ANL2 = ANL.copy(deep=True)\n",
    "for i in letters:\n",
    "    ONS2.usina = ONS2.usina.str.replace(i,letters.get(i))\n",
    "    ANL2.name = ANL2.name.str.replace(i,letters.get(i))\n",
    "for i in replace:\n",
    "    ONS2.usina = ONS2.usina.str.replace(i,replace.get(i))\n",
    "    ANL2.name = ANL2.name.str.replace(i,replace.get(i))\n",
    "for i in remove:\n",
    "    ONS2.usina = ONS2.usina.str.replace(i,remove.get(i))\n",
    "for i in numbers:\n",
    "    ONS2.usina = ONS2.usina.str.replace(i,numbers.get(i))\n",
    "    ANL2.name = ANL2.name.str.replace(i,numbers.get(i))\n",
    "\n",
    "# match windparks\n",
    "matches = ONS2.usina.apply(match_anl).apply(pd.Series)\n",
    "matches.columns = ['ONS_name','ANL_name','score']\n",
    "len(matches[matches.score<100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ONSd = pd.Series(ONS.usina.values,index=ONS2.usina.values)\n",
    "ANLd = pd.Series(ANL.name.values,index=ANL2.name.values)\n",
    "ONSd.columns = ['simpl','orig']\n",
    "ANLd.columns = ['simpl','orig']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### load simulated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# prepare simulated data as dataframe\n",
    "if (results_path + '/wpUSI_MER.pkl' not in glob.glob(results_path + '/*.pkl')):\n",
    "    wpERAxr = xr.open_dataset(results_path + '/windpower_stat_ERA5.nc',chunks={'time':80})\n",
    "    wpMERxr = xr.open_dataset(results_path + '/windpower_stat_MERRA2.nc',chunks={'time':80})\n",
    "    wpERAgxr = xr.open_mfdataset(results_path +'/windpower_??_ERA5_GWA.nc',chunks={'time':80})\n",
    "    wpMERgxr = xr.open_mfdataset(results_path +'/windpower_??_MERRA2_GWA.nc',chunks={'time':80})\n",
    "    \n",
    "    turb_mer = pd.read_csv(bra_path + '/turbine_data_mer.csv',index_col=0)\n",
    "    turb_era = pd.read_csv(bra_path + '/turbine_data_era.csv',index_col=0)\n",
    "    turb_merg = pd.read_csv(bra_path + '/turbine_data_mer_gwa3.csv',index_col=0)\n",
    "    turb_erag = pd.read_csv(bra_path + '/turbine_data_era_gwa3.csv',index_col=0)\n",
    "    \n",
    "    lbl = pd.read_csv(bra_path+ '/labels_turbine_data_gwa3.csv',index_col=0)\n",
    "    \n",
    "    wpMERdf = wpMERxr.to_dataframe().unstack().wp\n",
    "    wpERAdf = wpERAxr.to_dataframe().unstack().wp\n",
    "    wpMERgdf = wpMERgxr.assign_coords(location=range(len(wpMERgxr.location.values))).to_dataframe().unstack().wp\n",
    "    wpERAgdf = wpERAgxr.assign_coords(location=range(len(wpERAgxr.location.values))).to_dataframe().unstack().wp\n",
    "    # some locations have more than one park, get shares of parks\n",
    "    sharesMER = ANL.cap.groupby([lbl.lbl_mer.values,ANL.name.values]).sum()/ANL.cap.groupby([lbl.lbl_mer.values,ANL.name.values]).sum().index.get_level_values(0).map(ANL.cap.groupby(lbl.lbl_mer.values).sum())\n",
    "    sharesERA = ANL.cap.groupby([lbl.lbl_era.values,ANL.name.values]).sum()/ANL.cap.groupby([lbl.lbl_era.values,ANL.name.values]).sum().index.get_level_values(0).map(ANL.cap.groupby(lbl.lbl_era.values).sum())\n",
    "    sharesMERg = ANL.cap.groupby([lbl.lbl_mer_gwa.values,ANL.name.values]).sum()/ANL.cap.groupby([lbl.lbl_mer_gwa.values,ANL.name.values]).sum().index.get_level_values(0).map(ANL.cap.groupby(lbl.lbl_mer_gwa.values).sum())\n",
    "    sharesERAg = ANL.cap.groupby([lbl.lbl_era_gwa.values,ANL.name.values]).sum()/ANL.cap.groupby([lbl.lbl_era_gwa.values,ANL.name.values]).sum().index.get_level_values(0).map(ANL.cap.groupby(lbl.lbl_era_gwa.values).sum())\n",
    "    # get generation per park\n",
    "    wpMER = wpMERdf.loc[sharesMER.index.codes[0].values()].mul(sharesMER.values,axis=0).groupby(sharesMER.index.get_level_values(1).values).sum().transpose()\n",
    "    wpERA = wpERAdf.loc[sharesERA.index.codes[0].values()].mul(sharesERA.values,axis=0).groupby(sharesERA.index.get_level_values(1).values).sum().transpose()\n",
    "    wpMERg = wpMERgdf.loc[sharesMERg.index.codes[0].values()].mul(sharesMERg.values,axis=0).groupby(sharesMERg.index.get_level_values(1).values).sum().transpose()\n",
    "    wpERAg = wpERAgdf.loc[sharesERAg.index.codes[0].values()].mul(sharesERAg.values,axis=0).groupby(sharesERAg.index.get_level_values(1).values).sum().transpose()\n",
    "    # adapt index of MERRA data in 2019 (substract half an hour)\n",
    "    wpMER.index = wpMER.index[wpMER.index<'2018-12'].append(wpMER.index[wpMER.index>='2018-12'] - np.timedelta64(30,'m'))\n",
    "    wpMERg.index = wpMER.index[wpMERg.index<'2018-12'].append(wpMERg.index[wpMERg.index>='2018-12'] - np.timedelta64(30,'m'))\n",
    "    # set time zones\n",
    "    wpMER = wpMER.tz_localize('UTC').tz_convert('America/Bahia')\n",
    "    wpERA = wpERA.tz_localize('UTC').tz_convert('America/Bahia')\n",
    "    wpMERg = wpMERg.tz_localize('UTC').tz_convert('America/Bahia')\n",
    "    wpERAg = wpERAg.tz_localize('UTC').tz_convert('America/Bahia')\n",
    "    \n",
    "    wpMER.to_pickle(results_path + '/wpUSI_MER.pkl')\n",
    "    wpERA.to_pickle(results_path + '/wpUSI_ERA.pkl')\n",
    "    wpMERg.to_pickle(results_path + '/wpUSI_MERgwa.pkl')\n",
    "    wpERAg.to_pickle(results_path + '/wpUSI_ERAgwa.pkl')\n",
    "else:\n",
    "    wpMER = pd.read_pickle(results_path + '/wpUSI_MER.pkl')\n",
    "    wpERA = pd.read_pickle(results_path + '/wpUSI_ERA.pkl')\n",
    "    wpMERg = pd.read_pickle(results_path + '/wpUSI_MERgwa.pkl')\n",
    "    wpERAg = pd.read_pickle(results_path + '/wpUSI_ERAgwa.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### 0. remove leading and trailing 0s in observed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "(replace by nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "wpUSIhs[wpUSIhs.fillna(0).cumsum(axis=0)==0] = np.nan # remove leading 0s\n",
    "wpUSIhs[wpUSIhs[::-1].fillna(0).cumsum(axis=0)[::-1]==0] = np.nan # remove trailing 0s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 1. get  matching power generation timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "put ANEEL and ONS together into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "matches2 = pd.DataFrame({'ANL_name':matches.ANL_name.map(ANLd.drop_duplicates()),\n",
    "                         'ONS_name':matches.ONS_name.map(ONSd),\n",
    "                         'score':matches.score})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "get matching hourly windparks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "matches2H = matches2.copy(deep=True)\n",
    "matches2H = matches2H[[usi in wpUSIhs.columns.values for usi in matches2.ONS_name]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "save matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "matches2.to_pickle(bra_path + '/matches2.pkl')\n",
    "matches2H.to_pickle(bra_path + '/matches2H.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 2. remove constant timeseries from observed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def rm_constTS(wpt,lim=24):\n",
    "    '''\n",
    "    function for removing constant parts of time series\n",
    "     all series of more than lim (standard: 24 (hours))\n",
    "     are removed from the dataset\n",
    "    '''\n",
    "    wpt1 = wpt.copy(deep=True)\n",
    "    wpt1.index = wpt.index - np.timedelta64(1,'h')\n",
    "    # starts of constant timeseries\n",
    "    s = np.where((((wpt-wpt1).values[1:]==0).astype(int)-\n",
    "              ((wpt-wpt1).values[:-1]==0).astype(int))==1)[0]\n",
    "    # ends of constant timeseries\n",
    "    e = np.where((((wpt-wpt1).values[1:]==0).astype(int)-\n",
    "              ((wpt-wpt1).values[:-1]==0).astype(int))==-1)[0]\n",
    "    # filter starts and ends of rows of constant that are longer than 24 hours\n",
    "    sd = s[np.where((e-s)>lim)[0]]\n",
    "    ed = e[np.where((e-s)>lim)[0]]\n",
    "    #return(len(sd))\n",
    "    rmdf = pd.Series(0,index=wpt.index)\n",
    "    for i in range(len(sd)):\n",
    "        rmdf.iloc[sd[i]:ed[i]] = 1\n",
    "    return(wpt.where(rmdf==0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "wpUSIh = wpUSIhs.apply(rm_constTS,axis=0).tz_localize('America/Bahia',ambiguous=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(((wpUSIh.tz_localize(None)!=wpUSIhs).sum(axis=0)-wpUSIhs.isna().sum(axis=0))>0).sum()\n",
    "# part of data removed in 150 of 174 windparks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 3. separate short time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# get short time series (less than 2 years)\n",
    "USIhs = wpUSIh[wpUSIh.columns[wpUSIh.notna().sum(axis=0)<8760*2].values]\n",
    "matches2Hs = matches2H[[usi in USIhs.columns.values for usi in matches2H.ONS_name.values]]\n",
    "# get long time series (at least 2 years)\n",
    "USIhl = wpUSIh.drop(wpUSIh.columns[wpUSIh.notna().sum(axis=0)<8760*2].values,axis=1)\n",
    "matches2Hl = matches2H[[usi in USIhl.columns.values for usi in matches2H.ONS_name.values]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 4. find CFs > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def gcdH(park):\n",
    "    cap = ANL[ANL.name==park].cap.values\n",
    "    com = ANL[ANL.name==park].commissioning.astype(np.datetime64).values\n",
    "    return(get_cap_df(cap,com).tz_localize('UTC').tz_convert('America/Bahia'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# prepare/load capacities\n",
    "if gen_path+'/usi_capH_filterONS.pkl' not in glob.glob(gen_path + '/*'):\n",
    "    t1 = datetime.datetime.now()\n",
    "    capdf = pd.Series(ANL.name.unique()).apply(gcdH)\n",
    "    t2 = datetime.datetime.now()\n",
    "    print(t2-t1)\n",
    "    capdf.index = ANL.name.unique()\n",
    "    capdfH = capdf.transpose()\n",
    "    capdfH.to_pickle(gen_path+'/usi_capH_filterONS.pkl')\n",
    "capdfH = pd.read_pickle(gen_path+'/usi_capH_filterONS.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# get capacities of matching time series\n",
    "capUSIhs = capdfH[matches2Hs.ANL_name]\n",
    "capUSIhl = capdfH[matches2Hl.ANL_name]\n",
    "\n",
    "# adapt names\n",
    "capUSIhs.columns = matches2Hs.ONS_name.values\n",
    "capUSIhl.columns = matches2Hl.ONS_name.values\n",
    "\n",
    "# calculate capacity factors\n",
    "cfUSIhs = (USIhs[matches2Hs.ONS_name]/(capUSIhs/10**6))\n",
    "cfUSIhl = (USIhl[matches2Hl.ONS_name]/(capUSIhl/10**6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "12 parks where there are more than 10 days of CFs > 1 in hourly data (11 with matching score 100)\n",
    "\n",
    "remove those bad parks and for other with just a few CFs > 1 replace those CFs with nans (happens in statistics calculation function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "USIhlc = USIhl.drop(cfUSIhl.columns[((cfUSIhl>1).sum(axis=0)>10*24)].values,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "remove also from matching data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "matches2Hlc = matches2Hl.set_index('ONS_name').drop(cfUSIhl.columns[((cfUSIhl>1).sum(axis=0)>10*24)].values).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculate statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usinas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def analyseUSIh(parks):\n",
    "    compUSIh= pd.DataFrame({'MERRA2':wpMER[parks.ANL_name],\n",
    "                            'ERA5':wpERA[parks.ANL_name],\n",
    "                            'MERRA2_GWA':wpMERg[parks.ANL_name],\n",
    "                            'ERA5_GWA':wpERAg[parks.ANL_name],\n",
    "                            'wp_obs':wpUSIh[parks.ONS_name]*10**6})\n",
    "    # get capacities\n",
    "    capUSIh = capdfH[parks.ANL_name]\n",
    "    # calculate capacity factors\n",
    "    cf_USIh = compUSIh.div(capUSIh,axis=0)\n",
    "    # remove capacity factors > 1\n",
    "    cf_USIh = cf_USIh.mask(cf_USIh>1).dropna()\n",
    "    stat_h = pd.DataFrame({'ERA5':stats(cf_USIh.ERA5,cf_USIh.wp_obs,False),\n",
    "                           'ERA5_GWA':stats(cf_USIh.ERA5_GWA,cf_USIh.wp_obs,False),\n",
    "                           'MERRA2':stats(cf_USIh.MERRA2,cf_USIh.wp_obs,False),\n",
    "                           'MERRA2_GWA':stats(cf_USIh.MERRA2_GWA,cf_USIh.wp_obs,False),\n",
    "                           'obs':[np.nan,np.nan,np.nan,cf_USIh.wp_obs.mean()]},\n",
    "                          index = ['cor','rmse','mbe','avg']).reset_index().melt(id_vars=['index']).dropna()\n",
    "    stat_h.columns = ['param','dataset',parks.ANL_name]\n",
    "    return(stat_h.set_index(['param','dataset']).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def analyseUSId(parks):\n",
    "    compUSIh= pd.DataFrame({'MERRA2':wpMER[parks.ANL_name],\n",
    "                            'ERA5':wpERA[parks.ANL_name],\n",
    "                            'MERRA2_GWA':wpMERg[parks.ANL_name],\n",
    "                            'ERA5_GWA':wpERAg[parks.ANL_name],\n",
    "                            'wp_obs':wpUSIh[parks.ONS_name]*10**6})\n",
    "    # get capacities and mask\n",
    "    capUSIh = capdfH[parks.ANL_name].where(compUSIh.wp_obs.notna(),np.nan)\n",
    "    # mask\n",
    "    compUSIhm = compUSIh.where(compUSIh.wp_obs.notna(),np.nan,axis=1)\n",
    "    # aggregate daily\n",
    "    capUSId = capUSIh.resample('D').sum()\n",
    "    compUSId = compUSIhm.resample('D').sum()\n",
    "    # calculate capacity factors\n",
    "    cf_USId = compUSId.div(capUSId,axis=0)\n",
    "    # remove capacity factors > 1\n",
    "    cf_USId = cf_USId.mask(cf_USId>1).dropna()\n",
    "    stat_d = pd.DataFrame({'ERA5':stats(cf_USId.ERA5,cf_USId.wp_obs,False),\n",
    "                           'ERA5_GWA':stats(cf_USId.ERA5_GWA,cf_USId.wp_obs,False),\n",
    "                           'MERRA2':stats(cf_USId.MERRA2,cf_USId.wp_obs,False),\n",
    "                           'MERRA2_GWA':stats(cf_USId.MERRA2_GWA,cf_USId.wp_obs,False),\n",
    "                           'obs':[np.nan,np.nan,np.nan,cf_USId.wp_obs.mean()]},\n",
    "                          index = ['cor','rmse','mbe','avg']).reset_index().melt(id_vars=['index']).dropna()\n",
    "    stat_d.columns = ['param','dataset',parks.ANL_name]\n",
    "    return(stat_d.set_index(['param','dataset']).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def analyseUSIm(parks):\n",
    "    compUSIh= pd.DataFrame({'MERRA2':wpMER[parks.ANL_name],\n",
    "                            'ERA5':wpERA[parks.ANL_name],\n",
    "                            'MERRA2_GWA':wpMERg[parks.ANL_name],\n",
    "                            'ERA5_GWA':wpERAg[parks.ANL_name],\n",
    "                            'wp_obs':wpUSIh[parks.ONS_name]*10**6})\n",
    "    # get capacities and mask\n",
    "    capUSIh = capdfH[parks.ANL_name].where(compUSIh.wp_obs.notna(),np.nan)\n",
    "    # mask\n",
    "    compUSIhm = compUSIh.where(compUSIh.wp_obs.notna(),np.nan,axis=1)\n",
    "    # aggregate monthly\n",
    "    capUSIm = capUSIh.resample('M').sum()\n",
    "    compUSIm = compUSIhm.resample('M').sum()\n",
    "    # calculate capacity factors\n",
    "    cf_USIm = compUSIm.div(capUSIm,axis=0)\n",
    "    # remove capacity factors > 1\n",
    "    cf_USIm = cf_USIm.mask(cf_USIm>1).dropna()\n",
    "    stat_m = pd.DataFrame({'ERA5':stats(cf_USIm.ERA5,cf_USIm.wp_obs,False),\n",
    "                           'ERA5_GWA':stats(cf_USIm.ERA5_GWA,cf_USIm.wp_obs,False),\n",
    "                           'MERRA2':stats(cf_USIm.MERRA2,cf_USIm.wp_obs,False),\n",
    "                           'MERRA2_GWA':stats(cf_USIm.MERRA2_GWA,cf_USIm.wp_obs,False),\n",
    "                           'obs':[np.nan,np.nan,np.nan,cf_USIm.wp_obs.mean()]},\n",
    "                          index = ['cor','rmse','mbe','avg']).reset_index().melt(id_vars=['index']).dropna()\n",
    "    stat_m.columns = ['param','dataset',parks.ANL_name]\n",
    "    return(stat_m.set_index(['param','dataset']).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_USIh = pd.concat(matches2Hlc[matches2Hlc.score==100].apply(analyseUSIh,axis=1).tolist(),axis=0).transpose().dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_USId = pd.concat(matches2Hlc[matches2Hlc.score==100].apply(analyseUSId,axis=1).tolist(),axis=0).transpose().dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_USIm = pd.concat(matches2Hlc[matches2Hlc.score==100].apply(analyseUSIm,axis=1).tolist(),axis=0).transpose().dropna(axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "stats_USIh.to_csv(results_path + '/stats_USIh.csv')\n",
    "stats_USId.to_csv(results_path + '/stats_USId.csv')\n",
    "stats_USIm.to_csv(results_path + '/stats_USIm.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kgruber/.conda/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# insert states in matches dataframes\n",
    "matches2Hs['state'] = matches2Hs.ANL_name.map(ANL.groupby('name').state.first()).values\n",
    "matches2Hlc['state'] = matches2Hlc.ANL_name.map(ANL.groupby('name').state.first()).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def analyseESTh(usinas):\n",
    "    # remove leading and trailing 0s in observed data\n",
    "    wpobs = wpUSIh[usinas.ONS_name.values].sum(axis=1).copy(deep=True)\n",
    "    wpobs[wpobs.cumsum(axis=0)==0] = np.nan\n",
    "    wpobs[wpobs[::-1].cumsum(axis=0)[::-1]==0] = np.nan\n",
    "    # mask for masking simulated data and capacities\n",
    "    # (to only use timespans where also observed data are available)\n",
    "    mask = wpUSIh[usinas.ONS_name.values].notna()\n",
    "    mask.columns = usinas.ANL_name.values\n",
    "    # mask and aggregate simulated data\n",
    "    wpMER_ESTh = (wpMER[usinas.ANL_name.values]*mask).sum(axis=1)\n",
    "    wpERA_ESTh = (wpERA[usinas.ANL_name.values]*mask).sum(axis=1)\n",
    "    wpMERg_ESTh = (wpMERg[usinas.ANL_name.values]*mask).sum(axis=1)\n",
    "    wpERAg_ESTh = (wpERAg[usinas.ANL_name.values]*mask).sum(axis=1)\n",
    "    compESTh= pd.DataFrame({'MERRA2':wpMER_ESTh,\n",
    "                            'ERA5':wpERA_ESTh,\n",
    "                            'MERRA2_GWA':wpMERg_ESTh,\n",
    "                            'ERA5_GWA':wpERAg_ESTh,\n",
    "                            'wp_obs':wpobs*10**6})\n",
    "    # mask and aggregate capacities\n",
    "    capusish = capdfH[usinas.ANL_name.values]*mask\n",
    "    capESTh = capusish.sum(axis=1)\n",
    "    # calculate capacity factors\n",
    "    cf_ESThu = compESTh.div(capESTh,axis=0).dropna()\n",
    "    cf_ESTh = cf_ESThu.mask(cf_ESThu>1).dropna()\n",
    "    stat_h = pd.DataFrame({'ERA5':stats(cf_ESTh.ERA5,cf_ESTh.wp_obs,False),\n",
    "                           'ERA5_GWA':stats(cf_ESTh.ERA5_GWA,cf_ESTh.wp_obs,False),\n",
    "                           'MERRA2':stats(cf_ESTh.MERRA2,cf_ESTh.wp_obs,False),\n",
    "                           'MERRA2_GWA':stats(cf_ESTh.MERRA2_GWA,cf_ESTh.wp_obs,False),\n",
    "                           'obs':[np.nan,np.nan,np.nan,cf_ESTh.wp_obs.mean()]},\n",
    "                          index = ['cor','rmse','mbe','avg']).reset_index().melt(id_vars=['index']).dropna()\n",
    "    stat_h.columns = ['param','dataset',usinas.state.values[0]]\n",
    "    return(stat_h.set_index(['param','dataset']).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def analyseESTd(usinas):\n",
    "    # remove leading and trailing 0s in observed data\n",
    "    wpobs = wpUSIh[usinas.ONS_name.values].sum(axis=1).copy(deep=True)\n",
    "    wpobs[wpobs.cumsum(axis=0)==0] = np.nan\n",
    "    wpobs[wpobs[::-1].cumsum(axis=0)[::-1]==0] = np.nan\n",
    "    # mask for masking simulated data and capacities\n",
    "    # (to only use timespans where also observed data are available)\n",
    "    mask = wpUSIh[usinas.ONS_name.values].notna()\n",
    "    mask.columns = usinas.ANL_name.values\n",
    "    # mask and aggregate simulated data\n",
    "    wpMER_ESTh = (wpMER[usinas.ANL_name.values]*mask).sum(axis=1)\n",
    "    wpERA_ESTh = (wpERA[usinas.ANL_name.values]*mask).sum(axis=1)\n",
    "    wpMERg_ESTh = (wpMERg[usinas.ANL_name.values]*mask).sum(axis=1)\n",
    "    wpERAg_ESTh = (wpERAg[usinas.ANL_name.values]*mask).sum(axis=1)\n",
    "    compESTh= pd.DataFrame({'MERRA2':wpMER_ESTh,\n",
    "                            'ERA5':wpERA_ESTh,\n",
    "                            'MERRA2_GWA':wpMERg_ESTh,\n",
    "                            'ERA5_GWA':wpERAg_ESTh,\n",
    "                            'wp_obs':wpobs*10**6})\n",
    "    # mask and aggregate capacities\n",
    "    capusish = capdfH[usinas.ANL_name.values]*mask\n",
    "    capESTh = capusish.sum(axis=1)\n",
    "    # aggregate daily\n",
    "    compESTd = compESTh.resample('D').sum()\n",
    "    capESTd = capESTh.resample('D').sum()\n",
    "    # calculate capacity factors\n",
    "    cf_ESTdu = compESTd.div(capESTd,axis=0).dropna()\n",
    "    cf_ESTd = cf_ESTdu.mask(cf_ESTdu>1).dropna()\n",
    "    stat_d = pd.DataFrame({'ERA5':stats(cf_ESTd.ERA5,cf_ESTd.wp_obs,False),\n",
    "                           'ERA5_GWA':stats(cf_ESTd.ERA5_GWA,cf_ESTd.wp_obs,False),\n",
    "                           'MERRA2':stats(cf_ESTd.MERRA2,cf_ESTd.wp_obs,False),\n",
    "                           'MERRA2_GWA':stats(cf_ESTd.MERRA2_GWA,cf_ESTd.wp_obs,False),\n",
    "                           'obs':[np.nan,np.nan,np.nan,cf_ESTd.wp_obs.mean()]},\n",
    "                          index = ['cor','rmse','mbe','avg']).reset_index().melt(id_vars=['index']).dropna()\n",
    "    stat_d.columns = ['param','dataset',usinas.state.values[0]]\n",
    "    return(stat_d.set_index(['param','dataset']).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def analyseESTm(usinas):\n",
    "    # remove leading and trailing 0s in observed data\n",
    "    wpobs = wpUSIh[usinas.ONS_name.values].sum(axis=1).copy(deep=True)\n",
    "    wpobs[wpobs.cumsum(axis=0)==0] = np.nan\n",
    "    wpobs[wpobs[::-1].cumsum(axis=0)[::-1]==0] = np.nan\n",
    "    # mask for masking simulated data and capacities\n",
    "    # (to only use timespans where also observed data are available)\n",
    "    mask = wpUSIh[usinas.ONS_name.values].notna()\n",
    "    mask.columns = usinas.ANL_name.values\n",
    "    # mask and aggregate simulated data\n",
    "    wpMER_ESTh = (wpMER[usinas.ANL_name.values]*mask).sum(axis=1)\n",
    "    wpERA_ESTh = (wpERA[usinas.ANL_name.values]*mask).sum(axis=1)\n",
    "    wpMERg_ESTh = (wpMERg[usinas.ANL_name.values]*mask).sum(axis=1)\n",
    "    wpERAg_ESTh = (wpERAg[usinas.ANL_name.values]*mask).sum(axis=1)\n",
    "    compESTh= pd.DataFrame({'MERRA2':wpMER_ESTh,\n",
    "                            'ERA5':wpERA_ESTh,\n",
    "                            'MERRA2_GWA':wpMERg_ESTh,\n",
    "                            'ERA5_GWA':wpERAg_ESTh,\n",
    "                            'wp_obs':wpobs*10**6})\n",
    "    # mask and aggregate capacities\n",
    "    capusish = capdfH[usinas.ANL_name.values]*mask\n",
    "    capESTh = capusish.sum(axis=1)\n",
    "    # aggregate monthly\n",
    "    compESTm = compESTh.resample('M').sum()\n",
    "    capESTm = capESTh.resample('M').sum()\n",
    "    # calculate capacity factors\n",
    "    cf_ESTmu = compESTm.div(capESTm,axis=0).dropna()\n",
    "    cf_ESTm = cf_ESTmu.mask(cf_ESTmu>1).dropna()\n",
    "    stat_m = pd.DataFrame({'ERA5':stats(cf_ESTm.ERA5,cf_ESTm.wp_obs,False),\n",
    "                           'ERA5_GWA':stats(cf_ESTm.ERA5_GWA,cf_ESTm.wp_obs,False),\n",
    "                           'MERRA2':stats(cf_ESTm.MERRA2,cf_ESTm.wp_obs,False),\n",
    "                           'MERRA2_GWA':stats(cf_ESTm.MERRA2_GWA,cf_ESTm.wp_obs,False),\n",
    "                           'obs':[np.nan,np.nan,np.nan,cf_ESTm.wp_obs.mean()]},\n",
    "                          index = ['cor','rmse','mbe','avg']).reset_index().melt(id_vars=['index']).dropna()\n",
    "    stat_m.columns = ['param','dataset',usinas.state.values[0]]\n",
    "    return(stat_m.set_index(['param','dataset']).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_ESTh = matches2Hlc[matches2Hlc.score==100].groupby('state').apply(analyseESTh).transpose()\n",
    "stats_ESTh.columns = stats_ESTh.columns.get_level_values(0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_ESTd = matches2Hlc[matches2Hlc.score==100].groupby('state').apply(analyseESTd).transpose()\n",
    "stats_ESTd.columns = stats_ESTd.columns.get_level_values(0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_ESTm = matches2Hlc[matches2Hlc.score==100].groupby('state').apply(analyseESTm).transpose()\n",
    "stats_ESTm.columns = stats_ESTm.columns.get_level_values(0).values"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "stats_ESTh.to_csv(results_path + '/stats_ESTh.csv')\n",
    "stats_ESTd.to_csv(results_path + '/stats_ESTd.csv')\n",
    "stats_ESTm.to_csv(results_path + '/stats_ESTm.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brazil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def analyseBRAh(usinas):\n",
    "    # remove leading and trailing 0s in observed data\n",
    "    wpobs = wpUSIh[usinas.ONS_name.values].sum(axis=1).copy(deep=True)\n",
    "    wpobs[wpobs.cumsum(axis=0)==0] = np.nan\n",
    "    wpobs[wpobs[::-1].cumsum(axis=0)[::-1]==0] = np.nan\n",
    "    # mask for masking simulated data and capacities\n",
    "    # (to only use timespans where also observed data are available)\n",
    "    mask = wpUSIh[usinas.ONS_name.values].notna()\n",
    "    mask.columns = usinas.ANL_name.values\n",
    "    # mask and aggregate simulated data\n",
    "    wpMER_BRAh = (wpMER[usinas.ANL_name.values]*mask).sum(axis=1)\n",
    "    wpERA_BRAh = (wpERA[usinas.ANL_name.values]*mask).sum(axis=1)\n",
    "    wpMERg_BRAh = (wpMERg[usinas.ANL_name.values]*mask).sum(axis=1)\n",
    "    wpERAg_BRAh = (wpERAg[usinas.ANL_name.values]*mask).sum(axis=1)\n",
    "    compBRAh= pd.DataFrame({'MERRA2':wpMER_BRAh,\n",
    "                            'ERA5':wpERA_BRAh,\n",
    "                            'MERRA2_GWA':wpMERg_BRAh,\n",
    "                            'ERA5_GWA':wpERAg_BRAh,\n",
    "                            'wp_obs':wpobs*10**6})\n",
    "    # mask and aggregate capacities\n",
    "    capusish = capdfH[usinas.ANL_name.values]*mask\n",
    "    capBRAh = capusish.sum(axis=1)\n",
    "    # calculate capacity factors\n",
    "    cf_BRAhu = compBRAh.div(capBRAh,axis=0).dropna()\n",
    "    cf_BRAh = cf_BRAhu.mask(cf_BRAhu>1).dropna()\n",
    "    stat_h = pd.DataFrame({'ERA5':stats(cf_BRAh.ERA5,cf_BRAh.wp_obs,False),\n",
    "                           'ERA5_GWA':stats(cf_BRAh.ERA5_GWA,cf_BRAh.wp_obs,False),\n",
    "                           'MERRA2':stats(cf_BRAh.MERRA2,cf_BRAh.wp_obs,False),\n",
    "                           'MERRA2_GWA':stats(cf_BRAh.MERRA2_GWA,cf_BRAh.wp_obs,False),\n",
    "                           'obs':[np.nan,np.nan,np.nan,cf_BRAh.wp_obs.mean()]},\n",
    "                          index = ['cor','rmse','mbe','avg']).reset_index().melt(id_vars=['index']).dropna()\n",
    "    stat_h.columns = ['param','dataset','BRA']\n",
    "    return(stat_h.set_index(['param','dataset']).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def analyseBRAd(usinas):\n",
    "    # remove leading and trailing 0s in observed data\n",
    "    wpobs = wpUSIh[usinas.ONS_name.values].sum(axis=1).copy(deep=True)\n",
    "    wpobs[wpobs.cumsum(axis=0)==0] = np.nan\n",
    "    wpobs[wpobs[::-1].cumsum(axis=0)[::-1]==0] = np.nan\n",
    "    # mask for masking simulated data and capacities\n",
    "    # (to only use timespans where also observed data are available)\n",
    "    mask = wpUSIh[usinas.ONS_name.values].notna()\n",
    "    mask.columns = usinas.ANL_name.values\n",
    "    # mask and aggregate simulated data\n",
    "    wpMER_BRAh = (wpMER[usinas.ANL_name.values]*mask).sum(axis=1)\n",
    "    wpERA_BRAh = (wpERA[usinas.ANL_name.values]*mask).sum(axis=1)\n",
    "    wpMERg_BRAh = (wpMERg[usinas.ANL_name.values]*mask).sum(axis=1)\n",
    "    wpERAg_BRAh = (wpERAg[usinas.ANL_name.values]*mask).sum(axis=1)\n",
    "    compBRAh= pd.DataFrame({'MERRA2':wpMER_BRAh,\n",
    "                            'ERA5':wpERA_BRAh,\n",
    "                            'MERRA2_GWA':wpMERg_BRAh,\n",
    "                            'ERA5_GWA':wpERAg_BRAh,\n",
    "                            'wp_obs':wpobs*10**6})\n",
    "    # mask and aggregate capacities\n",
    "    capusish = capdfH[usinas.ANL_name.values]*mask\n",
    "    capBRAh = capusish.sum(axis=1)\n",
    "    # aggregate daily\n",
    "    compBRAd = compBRAh.resample('D').sum()\n",
    "    capBRAd = capBRAh.resample('D').sum()\n",
    "    # calculate capacity factors\n",
    "    cf_BRAdu = compBRAd.div(capBRAd,axis=0).dropna()\n",
    "    cf_BRAd = cf_BRAdu.mask(cf_BRAdu>1).dropna()\n",
    "    stat_d = pd.DataFrame({'ERA5':stats(cf_BRAd.ERA5,cf_BRAd.wp_obs,False),\n",
    "                           'ERA5_GWA':stats(cf_BRAd.ERA5_GWA,cf_BRAd.wp_obs,False),\n",
    "                           'MERRA2':stats(cf_BRAd.MERRA2,cf_BRAd.wp_obs,False),\n",
    "                           'MERRA2_GWA':stats(cf_BRAd.MERRA2_GWA,cf_BRAd.wp_obs,False),\n",
    "                           'obs':[np.nan,np.nan,np.nan,cf_BRAd.wp_obs.mean()]},\n",
    "                          index = ['cor','rmse','mbe','avg']).reset_index().melt(id_vars=['index']).dropna()\n",
    "    stat_d.columns = ['param','dataset','BRA']\n",
    "    return(stat_d.set_index(['param','dataset']).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def analyseBRAm(usinas):\n",
    "    # remove leading and trailing 0s in observed data\n",
    "    wpobs = wpUSIh[usinas.ONS_name.values].sum(axis=1).copy(deep=True)\n",
    "    wpobs[wpobs.cumsum(axis=0)==0] = np.nan\n",
    "    wpobs[wpobs[::-1].cumsum(axis=0)[::-1]==0] = np.nan\n",
    "    # mask for masking simulated data and capacities\n",
    "    # (to only use timespans where also observed data are available)\n",
    "    mask = wpUSIh[usinas.ONS_name.values].notna()\n",
    "    mask.columns = usinas.ANL_name.values\n",
    "    # mask and aggregate simulated data\n",
    "    wpMER_BRAh = (wpMER[usinas.ANL_name.values]*mask).sum(axis=1)\n",
    "    wpERA_BRAh = (wpERA[usinas.ANL_name.values]*mask).sum(axis=1)\n",
    "    wpMERg_BRAh = (wpMERg[usinas.ANL_name.values]*mask).sum(axis=1)\n",
    "    wpERAg_BRAh = (wpERAg[usinas.ANL_name.values]*mask).sum(axis=1)\n",
    "    compBRAh= pd.DataFrame({'MERRA2':wpMER_BRAh,\n",
    "                            'ERA5':wpERA_BRAh,\n",
    "                            'MERRA2_GWA':wpMERg_BRAh,\n",
    "                            'ERA5_GWA':wpERAg_BRAh,\n",
    "                            'wp_obs':wpobs*10**6})\n",
    "    # mask and aggregate capacities\n",
    "    capusish = capdfH[usinas.ANL_name.values]*mask\n",
    "    capBRAh = capusish.sum(axis=1)\n",
    "    # aggregate monthly\n",
    "    compBRAm = compBRAh.resample('M').sum()\n",
    "    capBRAm = capBRAh.resample('M').sum()\n",
    "    # calculate capacity factors\n",
    "    cf_BRAmu = compBRAm.div(capBRAm,axis=0).dropna()\n",
    "    cf_BRAm = cf_BRAmu.mask(cf_BRAmu>1).dropna()\n",
    "    stat_m = pd.DataFrame({'ERA5':stats(cf_BRAm.ERA5,cf_BRAm.wp_obs,False),\n",
    "                           'ERA5_GWA':stats(cf_BRAm.ERA5_GWA,cf_BRAm.wp_obs,False),\n",
    "                           'MERRA2':stats(cf_BRAm.MERRA2,cf_BRAm.wp_obs,False),\n",
    "                           'MERRA2_GWA':stats(cf_BRAm.MERRA2_GWA,cf_BRAm.wp_obs,False),\n",
    "                           'obs':[np.nan,np.nan,np.nan,cf_BRAm.wp_obs.mean()]},\n",
    "                          index = ['cor','rmse','mbe','avg']).reset_index().melt(id_vars=['index']).dropna()\n",
    "    stat_m.columns = ['param','dataset','BRA']\n",
    "    return(stat_m.set_index(['param','dataset']).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_BRAh = analyseBRAh(matches2Hlc[matches2Hlc.score==100]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_BRAd = analyseBRAd(matches2Hlc[matches2Hlc.score==100]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_BRAm = analyseBRAm(matches2Hlc[matches2Hlc.score==100]).transpose()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "stats_BRAh.to_csv(results_path + '/stats_BRAh.csv')\n",
    "stats_BRAd.to_csv(results_path + '/stats_BRAd.csv')\n",
    "stats_BRAm.to_csv(results_path + '/stats_BRAm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merge results and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sUSIh = stats_USIh.reset_index().melt(id_vars=['param','dataset'], value_vars=stats_USIh.columns, var_name='location')\n",
    "sUSIh['temp'] = 'h'\n",
    "sUSIh['scale'] = 'park'\n",
    "sUSId = stats_USId.reset_index().melt(id_vars=['param','dataset'], value_vars=stats_USId.columns, var_name='location')\n",
    "sUSId['temp'] = 'd'\n",
    "sUSId['scale'] = 'park'\n",
    "sUSIm = stats_USIm.reset_index().melt(id_vars=['param','dataset'], value_vars=stats_USIm.columns, var_name='location')\n",
    "sUSIm['temp'] = 'm'\n",
    "sUSIm['scale'] = 'park'\n",
    "sESTh = stats_ESTh.reset_index().melt(id_vars=['param','dataset'], value_vars=stats_ESTh.columns, var_name='location')\n",
    "sESTh['temp'] = 'h'\n",
    "sESTh['scale'] = 'state'\n",
    "sESTd = stats_ESTd.reset_index().melt(id_vars=['param','dataset'], value_vars=stats_ESTd.columns, var_name='location')\n",
    "sESTd['temp'] = 'd'\n",
    "sESTd['scale'] = 'state'\n",
    "sESTm = stats_ESTm.reset_index().melt(id_vars=['param','dataset'], value_vars=stats_ESTm.columns, var_name='location')\n",
    "sESTm['temp'] = 'm'\n",
    "sESTm['scale'] = 'state'\n",
    "sBRAh = stats_BRAh.reset_index().melt(id_vars=['param','dataset'], value_vars=stats_BRAh.columns, var_name='location')\n",
    "sBRAh['temp'] = 'h'\n",
    "sBRAh['scale'] = 'country'\n",
    "sBRAd = stats_BRAd.reset_index().melt(id_vars=['param','dataset'], value_vars=stats_BRAd.columns, var_name='location')\n",
    "sBRAd['temp'] = 'd'\n",
    "sBRAd['scale'] = 'country'\n",
    "sBRAm = stats_BRAm.reset_index().melt(id_vars=['param','dataset'], value_vars=stats_BRAm.columns, var_name='location')\n",
    "sBRAm['temp'] = 'm'\n",
    "sBRAm['scale'] = 'country'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = pd.concat([sUSIh, sESTh, sBRAh,\n",
    "                   sUSId, sESTd, sBRAd,\n",
    "                   sUSIm, sESTm, sBRAm], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.to_csv(results_path + '/stats_GWA3.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-py37]",
   "language": "python",
   "name": "conda-env-.conda-py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
