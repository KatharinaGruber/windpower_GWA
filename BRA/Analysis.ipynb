{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from dateutil.relativedelta import *\n",
    "from fuzzywuzzy import fuzz\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_1samp\n",
    "import sys\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "from paths_bra import *\n",
    "\n",
    "sys.path.append('./..')\n",
    "from refuelplot import *\n",
    "setup()\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_path = bra_path + '/generation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load generation data\n",
    "# load usinas hourly\n",
    "if gen_path + '/hourly/usinas.pkl' not in glob.glob(gen_path + '/hourly/*.pkl'):\n",
    "    USIh = pd.read_csv(gen_path + '/hourly/Comparativo_Geração_de_Energia_Semana_data_usinas.csv',\n",
    "                       sep = ';', index_col = 0, parse_dates = True, dayfirst = True).iloc[1:,[6,8]].sort_index()\n",
    "    # remove missing values\n",
    "    USIh = USIh.loc[USIh.index.notnull()].dropna()\n",
    "    USIh.columns = ['usina','prod_GWh']\n",
    "\n",
    "    # in RIO DO FOGO there is one duplicate hour after one missing hour -> change timestamps of those hours\n",
    "    idxUSIh = USIh.index.values\n",
    "    midxUSIh = USIh.reset_index().set_index(['usina','Data Escala de Tempo 1 GE Comp 3']).index\n",
    "    idxUSIh[midxUSIh.duplicated(keep='last')]  = idxUSIh[midxUSIh.duplicated(keep='first')] - np.timedelta64(1,'h')\n",
    "    USIh.index = pd.DatetimeIndex(idxUSIh)\n",
    "\n",
    "    USIhs = USIh.reset_index().set_index(['usina','index']).unstack(level=0).prod_GWh\n",
    "    USIhs.to_csv(gen_path + '/hourly/usinas.csv')\n",
    "    USIhs.to_pickle(gen_path + '/hourly/usinas.pkl')\n",
    "#USIhs = pd.read_csv(gen_path + '/hourly/usinas.csv',index_col=0,parse_dates=True)\n",
    "USIhs = pd.read_pickle(gen_path + '/hourly/usinas.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load usinas monthly\n",
    "if gen_path + '/monthly/usinas.pkl' not in glob.glob(gen_path + '/monthly/*.pkl'):\n",
    "    USIm = pd.read_csv(gen_path + '/monthly/Comparativo_Geração_de_Energia_Semana_data_usinas.csv',\n",
    "                         sep = ';', index_col = 0, parse_dates = True, dayfirst = True).iloc[1:,[6,8]].sort_index()\n",
    "    # remove missing values\n",
    "    USIm = USIm.loc[USIm.index.notnull()].dropna()\n",
    "    USIm.columns = ['usina','prod_GWh']\n",
    "\n",
    "    USIms = USIm.reset_index().set_index(['usina','Data Escala de Tempo 1 GE Comp 3']).unstack(level=0).prod_GWh\n",
    "    USIms.to_csv(gen_path + '/monthly/usinas.csv')\n",
    "    USIms.to_pickle(gen_path + '/monthly/usinas.pkl')\n",
    "#USIms = pd.read_csv(gen_path + '/monthly/usinas.csv',index_col=0,parse_dates=True)\n",
    "USIms = pd.read_pickle(gen_path + '/monthly/usinas.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "USIhs[USIhs.fillna(0).cumsum(axis=0)==0] = np.nan # remove leading 0s\n",
    "USIhs[USIhs[::-1].fillna(0).cumsum(axis=0)[::-1]==0] = np.nan # remove trailing 0s\n",
    "\n",
    "USIms[USIms.fillna(0).cumsum(axis=0)==0] = np.nan # remove leading 0s\n",
    "USIms[USIms[::-1].fillna(0).cumsum(axis=0)[::-1]==0] = np.nan # remove trailing 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cap_df(cap,comdate):\n",
    "    com = pd.DataFrame({'capacity': cap}).groupby(comdate).sum()\n",
    "    cap_cum = com.capacity.cumsum()\n",
    "    # if only years given for commissioning dates -> gradual capacity increase over year, full capacity at end of year\n",
    "    if type(cap_cum.index.values[0]) == np.int64:\n",
    "        cap_cum.index = [np.datetime64(str(int(year))+\"-12-31 23:00:00\") for year in cap_cum.index.values]\n",
    "        # create yearly dates at yearends\n",
    "        drcc = pd.date_range(np.datetime64('2005-12-31 23:00:00'),\n",
    "                             np.datetime64('2019-12-31 23:00:00'),freq= 'y')\n",
    "        cap_cum = pd.Series(drcc.map(cap_cum),index = drcc)\n",
    "        # if first year emtpy: either year before or 0 if nothing before\n",
    "        if(sum(com.index<2000) > 0):\n",
    "            cap_cum[0] = com.cumsum()[com.index<2000].max()\n",
    "        else:\n",
    "            cap_cum[0] = 0\n",
    "        # if missing years -> put capacity of year before\n",
    "        cap_cum = cap_cum.ffill()\n",
    "    dr = pd.date_range('1/1/2006','31/12/2019 23:00:00',freq = 'h')\n",
    "    cap_ts = pd.Series(dr.map(cap_cum),index = dr)\n",
    "    cap_ts[0] = cap_cum[cap_cum.index<=pd.Timestamp('2006-01-01')].max()\n",
    "    if type(comdate[0]) == np.int64:\n",
    "        return(cap_ts.interpolate(method='linear'))\n",
    "    else:\n",
    "        return(cap_ts.fillna(method='ffill'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     32,
     53,
     73,
     108
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load and match aneel and ons windparks\n",
    "def matchWords(word, statements):\n",
    "    # function to match a word to different statements\n",
    "    # output: ratio of matching (0-100) for all provided statements\n",
    "    results = []\n",
    "    for s in statements:\n",
    "        r = fuzz.ratio(word, s)\n",
    "        results.append(r)\n",
    "    return results\n",
    "\n",
    "def match_string(string, array):\n",
    "    # function for matching casefolded strings\n",
    "    Slc = string.strip().casefold()\n",
    "    Alc = [arr.casefold() for arr in array.str.strip().unique()]\n",
    "    scores = matchWords(Slc, Alc)\n",
    "    mscore = max(scores)\n",
    "    strarr = array.unique()[np.where(np.array(scores)==mscore)][0]\n",
    "    return(string,strarr,mscore)\n",
    "\n",
    "def match_anl(string):\n",
    "    # function to match ONS to ANL windparks\n",
    "    return(match_string(string,ANL2.name))\n",
    "\n",
    "# load ANEEL and ONS windparks\n",
    "ONS = pd.read_csv(bra_path + '/ONS_windparks.csv', index_col = 0)\n",
    "# remove those with CONJUNTO EOLICO - they're there twice and capacities don't match with ANEEL data\n",
    "ONS = ONS[~ONS.usina.str.contains('CONJUNTO EOLICO')]\n",
    "# remove some other duplicate windparks\n",
    "ONS = ONS[[d not in [' CANOA QUEBRADA (E-RV-ACEP)',' PV DO NORDESTE',' SM (SANTA MARIA)',' SÃO BENTO NORTE II'] for d in ONS.usina]]\n",
    "ANL = pd.read_csv(bra_path + '/turbine_data.csv', index_col = 0)\n",
    "\n",
    "# characters and strings to replace for better matching\n",
    "letters = {'Ãµ':'õ',\n",
    "           'ó':'o',\n",
    "           'ã':'a',\n",
    "           'á':'a',\n",
    "           'â':'a',\n",
    "           'é':'e',\n",
    "           'Ã':'A',\n",
    "           'Á':'A',\n",
    "           'Â':'A',\n",
    "           'Ó':'O',\n",
    "           'É':'E',\n",
    "           'ú':'u',\n",
    "           'ô':'o',\n",
    "           'Ô':'O',\n",
    "           'ú':'u',\n",
    "           'Ú':'U',\n",
    "           'ç':'c',\n",
    "           'Ç':'C',\n",
    "           'í':'i',\n",
    "           'Í':'I',\n",
    "           'Ê':'E'}\n",
    "remove = {' 2LER':'',\n",
    "          ' 2LFA':'',\n",
    "          ' LFA':'',\n",
    "          'EOL ':'',\n",
    "          ' 3LER':'',\n",
    "          'Usina Eolica ':'',\n",
    "          'Eólica ':'',\n",
    "          ' ENERGIAS RENOVAVEIS':'',\n",
    "#          ' CONJUNTO EOLICO':'',\n",
    "          '\\(E-BV-ACEP\\)':'',\n",
    "          '\\(E-RV-ACEP\\)':'',\n",
    "          '\\(BELA BISTA\\)':'',\n",
    "          '\\(ENERGEN\\)':'',\n",
    "          '\\(Antiga Ventos Maranhenses 05\\)':'',\n",
    "          'PARQUE EOLICO ':'',\n",
    "          ' - N HORIZ':'',\n",
    "          'ENERGETICA S/A':'',\n",
    "          '\\(ILHEUS\\)':'',\n",
    "          ' EOLOS':'',\n",
    "          'S\\.A\\.':''}\n",
    "replace = {'LAG DO':'LAGOA DO',\n",
    "           'VENTOS S VICENTE':'VENTOS DE SAO VICENTE',\n",
    "           'SERRA BABILONIA':'SERRA DA BABILONIA',\n",
    "           'CORREDOR SENANDES':'CORREDOR DO SENANDES',\n",
    "           'SAO BENTO NORTE':'SAO BENTO DO NORTE',\n",
    "           'GAMELEIRAS':'GAMELERIAS',\n",
    "           'Lagoinha':'Lagoinh',\n",
    "           'PAPAGAIOS':'PAPAGAIO',\n",
    "           'VENTOS DE SAO ABRAAO':'VENTOS DO SANTO ABRAAO',\n",
    "           'VENTOS DO SAO MARIO':'VENTOS DE SAO MARIO',\n",
    "           'DAGUA':'D AGUA',\n",
    "           'B VEN':'BONS VENTOS',\n",
    "           'NOVA BURITI':'BURITI',\n",
    "           'NOVA CAJUCOCO':'CAJUCOCO',\n",
    "           'PALMAS':'DE PALMAS',\n",
    "           'DE PALMARES':'PALMARES',\n",
    "           'PV DO NORDESTE':'VENTOS DO NORDESTE',\n",
    "           'Aura Lagoa do Barro':'Lagoa do Barro',\n",
    "           'AURA LAGOA DO BARRO':'LAGOA DO BARRO',\n",
    "           'LAGOA BARRO':'LAGOA DO BARRO',\n",
    "           'GRAVATA':'GRAVATA FRUITRADE',\n",
    "           'FAZENDA DO ROSARIO':'FAZENDA ROSARIO',\n",
    "           'Parque Eolico do Horizonte':'Ventos de Horizonte',\n",
    "           'S BENTO':'SAO BENTO',\n",
    "           'SANTO ANTONIO (BTG PACTUAL)':'SANTO ANTONIO DE PADUA',\n",
    "           'SM \\(SANTA MARIA\\)':'SANTA MARIA',\n",
    "           'SAO JORGE CE':'SAO JORGE',\n",
    "           'VENT DA ST ESPERANCA':'VENTOS DA SANTA ESPERANCA',\n",
    "           'VENTOS DA STA DULCE':'VENTOS DA SANTA DULCE',\n",
    "           'ESPERANCA NORDESTE':'ESPERANCA DO NORDESTE',\n",
    "           'Eolica Delta':'Delta',\n",
    "           'Eolica Serra das Vacas':'Serra das Vacas',\n",
    "           'Ventos de Santo Augusto':'Santo Augusto',\n",
    "           'Ventos do Sao Gabriel':'Sao Gabriel',\n",
    "           'GE Maria Helena':'Maria Helena'}\n",
    "numbers = {'10':'X',\n",
    "           '11':'XI',\n",
    "           '12':'XII',\n",
    "           '13':'XIII',\n",
    "           '14':'XIV',\n",
    "           '15':'XV',\n",
    "           '17':'XVII',\n",
    "           '19':'XIX',\n",
    "           '21':'XXI',\n",
    "           '23':'XXIII',\n",
    "           '24':'XXIV',\n",
    "           '25':'XXV',\n",
    "           '26':'XXVI',\n",
    "           '27':'XXVII',\n",
    "           '28':'XXVIII',\n",
    "           '29':'XXIX',\n",
    "           '31':'XXXI',\n",
    "           '34':'XXXIV',\n",
    "           '35':'XXXV',\n",
    "           '36':'XXXVI',\n",
    "           '01':'I',\n",
    "           '02':'II',\n",
    "           '03':'III',\n",
    "           '04':'IV',\n",
    "           '05':'V',\n",
    "           '06':'VI',\n",
    "           '07':'VII',\n",
    "           '08':'VIII',\n",
    "           '09':'IX',\n",
    "           '1':'I',\n",
    "           '2':'II',\n",
    "           '3':'III',\n",
    "           '4':'IV',\n",
    "           '5':'V',\n",
    "           '6':'VI',\n",
    "           '7':'VII',\n",
    "           '8':'VIII',\n",
    "           '9':'IX'}\n",
    "\n",
    "# replace characters\n",
    "ONS2 = ONS.copy(deep=True)\n",
    "ANL2 = ANL.copy(deep=True)\n",
    "for i in letters:\n",
    "    ONS2.usina = ONS2.usina.str.replace(i,letters.get(i))\n",
    "    ANL2.name = ANL2.name.str.replace(i,letters.get(i))\n",
    "for i in replace:\n",
    "    ONS2.usina = ONS2.usina.str.replace(i,replace.get(i))\n",
    "    ANL2.name = ANL2.name.str.replace(i,replace.get(i))\n",
    "for i in remove:\n",
    "    ONS2.usina = ONS2.usina.str.replace(i,remove.get(i))\n",
    "for i in numbers:\n",
    "    ONS2.usina = ONS2.usina.str.replace(i,numbers.get(i))\n",
    "    ANL2.name = ANL2.name.str.replace(i,numbers.get(i))\n",
    "\n",
    "# match windparks\n",
    "matches = ONS2.usina.apply(match_anl).apply(pd.Series)\n",
    "matches.columns = ['ONS_name','ANL_name','score']\n",
    "len(matches[matches.score<100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ONSd = pd.Series(ONS.usina.values,index=ONS2.usina.values)#.reset_index().drop_duplicates()\n",
    "ANLd = pd.Series(ANL.name.values,index=ANL2.name.values)#.reset_index().drop_duplicates()\n",
    "ONSd.columns = ['simpl','orig']\n",
    "ANLd.columns = ['simpl','orig']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches2 = pd.DataFrame({'ANL_name':matches.ANL_name.map(ANLd.drop_duplicates()),\n",
    "                         'ONS_name':matches.ONS_name.map(ONSd),\n",
    "                         'score':matches.score})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get only matching power generation timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpUSIm = USIms[matches2.ONS_name[matches2.score==100].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches2H = matches2[[usi in USIhs.columns.values for usi in matches2.ONS_name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpUSIh = USIhs[matches2H.ONS_name.values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load simualted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpERAxr = xr.open_dataset(results_path + '/windpower_stat_ERA5.nc')\n",
    "wpMERxr = xr.open_dataset(results_path + '/windpower_stat_MERRA2.nc')\n",
    "wpERAgxr = xr.open_mfdataset(results_path +'/windpower_??_ERA5_GWA.nc')\n",
    "wpMERgxr = xr.open_mfdataset(results_path +'/windpower_??_MERRA2_GWA.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "turb_mer = pd.read_csv(bra_path + '/turbine_data_mer.csv',index_col=0)\n",
    "turb_era = pd.read_csv(bra_path + '/turbine_data_era.csv',index_col=0)\n",
    "turb_merg = pd.read_csv(bra_path + '/turbine_data_mer_gwa3.csv',index_col=0)\n",
    "turb_erag = pd.read_csv(bra_path + '/turbine_data_era_gwa3.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl = pd.read_csv(bra_path+ '/labels_turbine_data_gwa3.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare simulated data as dataframe\n",
    "wpMERdf = wpMERxr.to_dataframe().unstack().wp\n",
    "wpERAdf = wpERAxr.to_dataframe().unstack().wp\n",
    "wpMERgdf = wpMERgxr.assign_coords(location=range(len(wpMERgxr.location.values))).to_dataframe().unstack().wp\n",
    "wpERAgdf = wpERAgxr.assign_coords(location=range(len(wpERAgxr.location.values))).to_dataframe().unstack().wp\n",
    "# some locations have more than one park, get shares of parks\n",
    "sharesMER = ANL.cap.groupby([lbl.lbl_mer.values,ANL.name.values]).sum()/ANL.cap.groupby([lbl.lbl_mer.values,ANL.name.values]).sum().index.get_level_values(0).map(ANL.cap.groupby(lbl.lbl_mer.values).sum())\n",
    "sharesERA = ANL.cap.groupby([lbl.lbl_era.values,ANL.name.values]).sum()/ANL.cap.groupby([lbl.lbl_era.values,ANL.name.values]).sum().index.get_level_values(0).map(ANL.cap.groupby(lbl.lbl_era.values).sum())\n",
    "sharesMERg = ANL.cap.groupby([lbl.lbl_mer_gwa.values,ANL.name.values]).sum()/ANL.cap.groupby([lbl.lbl_mer_gwa.values,ANL.name.values]).sum().index.get_level_values(0).map(ANL.cap.groupby(lbl.lbl_mer_gwa.values).sum())\n",
    "sharesERAg = ANL.cap.groupby([lbl.lbl_era_gwa.values,ANL.name.values]).sum()/ANL.cap.groupby([lbl.lbl_era_gwa.values,ANL.name.values]).sum().index.get_level_values(0).map(ANL.cap.groupby(lbl.lbl_era_gwa.values).sum())\n",
    "# get generation per park\n",
    "wpMER = wpMERdf.loc[sharesMER.index.codes[0].values()].mul(sharesMER.values,axis=0).groupby(sharesMER.index.get_level_values(1).values).sum().transpose()\n",
    "wpERA = wpERAdf.loc[sharesERA.index.codes[0].values()].mul(sharesERA.values,axis=0).groupby(sharesERA.index.get_level_values(1).values).sum().transpose()\n",
    "wpMERg = wpMERgdf.loc[sharesMERg.index.codes[0].values()].mul(sharesMERg.values,axis=0).groupby(sharesMERg.index.get_level_values(1).values).sum().transpose()\n",
    "wpERAg = wpERAgdf.loc[sharesERAg.index.codes[0].values()].mul(sharesERAg.values,axis=0).groupby(sharesERAg.index.get_level_values(1).values).sum().transpose()\n",
    "# adapt index of MERRA data in 2019 (substract half an hour)\n",
    "wpMER.index = wpMER.index[wpMER.index<'2018-12'].append(wpMER.index[wpMER.index>='2018-12'] - np.timedelta64(30,'m'))\n",
    "wpMERg.index = wpMER.index[wpMERg.index<'2018-12'].append(wpMERg.index[wpMERg.index>='2018-12'] - np.timedelta64(30,'m'))\n",
    "# set time zones\n",
    "wpMER = wpMER.tz_localize('UTC').tz_convert('Etc/GMT-3')\n",
    "wpERA = wpERA.tz_localize('UTC').tz_convert('Etc/GMT-3')\n",
    "wpMERg = wpMERg.tz_localize('UTC').tz_convert('Etc/GMT-3')\n",
    "wpERAg = wpERAg.tz_localize('UTC').tz_convert('Etc/GMT-3')\n",
    "wpUSIh = wpUSIh.tz_localize('Etc/GMT-3')\n",
    "wpUSIm = wpUSIm.tz_localize('Etc/GMT-3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyseUSIh(parks):\n",
    "    compUSIh= pd.DataFrame({'MERRA2':wpMER[parks.ANL_name],\n",
    "                            'ERA5':wpERA[parks.ANL_name],\n",
    "                            'MERRA2_GWA':wpMERg[parks.ANL_name],\n",
    "                            'ERA5_GWA':wpERAg[parks.ANL_name],\n",
    "                            'wp_obs':wpUSIh[parks.ONS_name]*10**6})\n",
    "    # get capacities\n",
    "    capUSIh = get_cap_df(ANL[ANL.name==parks.ANL_name].cap.values,\n",
    "                         ANL[ANL.name==parks.ANL_name].commissioning.astype(np.datetime64).values).tz_localize('UTC').tz_convert('Etc/GMT-3')\n",
    "    # calculate capacity factors\n",
    "    cf_USIh = compUSIh.div(capUSIh,axis=0).dropna()\n",
    "    stat_h = pd.DataFrame({'ERA5':stats(cf_USIh.ERA5,cf_USIh.wp_obs,False),\n",
    "                           'ERA5_GWA':stats(cf_USIh.ERA5_GWA,cf_USIh.wp_obs,False),\n",
    "                           'MERRA2':stats(cf_USIh.MERRA2,cf_USIh.wp_obs,False),\n",
    "                           'MERRA2_GWA':stats(cf_USIh.MERRA2_GWA,cf_USIh.wp_obs,False),\n",
    "                           'obs':[np.nan,np.nan,np.nan,cf_USIh.wp_obs.mean()]},\n",
    "                          index = ['cor','rmse','mbe','avg']).reset_index().melt(id_vars=['index']).dropna()\n",
    "    stat_h.columns = ['param','dataset',parks.ANL_name]\n",
    "    return(stat_h.set_index(['param','dataset']).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyseUSId(parks):\n",
    "    compUSIh= pd.DataFrame({'MERRA2':wpMER[parks.ANL_name],\n",
    "                            'ERA5':wpERA[parks.ANL_name],\n",
    "                            'MERRA2_GWA':wpMERg[parks.ANL_name],\n",
    "                            'ERA5_GWA':wpERAg[parks.ANL_name],\n",
    "                            'wp_obs':wpUSIh[parks.ONS_name]*10**6})\n",
    "    # get capacities\n",
    "    capUSIh = get_cap_df(ANL[ANL.name==parks.ANL_name].cap.values,\n",
    "                         ANL[ANL.name==parks.ANL_name].commissioning.astype(np.datetime64).values).tz_localize('UTC').tz_convert('Etc/GMT-3')\n",
    "    ccUSIh = pd.concat([compUSIh,capUSIh],axis=1).dropna()\n",
    "    ccUSIh.columns = compUSIh.columns.tolist() + ['cap']\n",
    "    # aggregate daily\n",
    "    ccUSId = ccUSIh.resample('D').sum()\n",
    "    cf_USId = ccUSId.drop('cap',axis=1).div(ccUSId.cap,axis=0)\n",
    "    stat_d = pd.DataFrame({'ERA5':stats(cf_USId.ERA5,cf_USId.wp_obs,False),\n",
    "                           'ERA5_GWA':stats(cf_USId.ERA5_GWA,cf_USId.wp_obs,False),\n",
    "                           'MERRA2':stats(cf_USId.MERRA2,cf_USId.wp_obs,False),\n",
    "                           'MERRA2_GWA':stats(cf_USId.MERRA2_GWA,cf_USId.wp_obs,False),\n",
    "                           'obs':[np.nan,np.nan,np.nan,cf_USId.wp_obs.mean()]},\n",
    "                          index = ['cor','rmse','mbe','avg']).reset_index().melt(id_vars=['index']).dropna()\n",
    "    stat_d.columns = ['param','dataset',parks.ANL_name]\n",
    "    return(stat_d.set_index(['param','dataset']).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_USIh = pd.concat(matches2H[matches2H.score==100].apply(analyseUSIh,axis=1).tolist(),axis=0).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_USId = pd.concat(matches2H[matches2H.score==100].apply(analyseUSId,axis=1).tolist(),axis=0).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-py37]",
   "language": "python",
   "name": "conda-env-.conda-py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
