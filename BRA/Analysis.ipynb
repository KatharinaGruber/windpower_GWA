{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from dateutil.relativedelta import *\n",
    "from fuzzywuzzy import fuzz\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_1samp\n",
    "import sys\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "from paths_bra import *\n",
    "\n",
    "sys.path.append('./..')\n",
    "from refuelplot import *\n",
    "setup()\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_path = bra_path + '/generation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## load generation data\n",
    "# load usinas hourly\n",
    "if gen_path + '/hourly/usinas.pkl' not in glob.glob(gen_path + '/hourly/*.pkl'):\n",
    "    USIh = pd.read_csv(gen_path + '/hourly/Comparativo_Geração_de_Energia_Semana_data_usinas.csv',\n",
    "                       sep = ';', index_col = 0, parse_dates = True, dayfirst = True).iloc[1:,[6,8]].sort_index()\n",
    "    # remove missing values\n",
    "    USIh = USIh.loc[USIh.index.notnull()].dropna()\n",
    "    USIh.columns = ['usina','prod_GWh']\n",
    "\n",
    "    # in RIO DO FOGO there is one duplicate hour after one missing hour -> change timestamps of those hours\n",
    "    idxUSIh = USIh.index.values\n",
    "    midxUSIh = USIh.reset_index().set_index(['usina','Data Escala de Tempo 1 GE Comp 3']).index\n",
    "    idxUSIh[midxUSIh.duplicated(keep='last')]  = idxUSIh[midxUSIh.duplicated(keep='first')] - np.timedelta64(1,'h')\n",
    "    USIh.index = pd.DatetimeIndex(idxUSIh)\n",
    "\n",
    "    USIhs = USIh.reset_index().set_index(['usina','index']).unstack(level=0).prod_GWh\n",
    "    USIhs.to_csv(gen_path + '/hourly/usinas.csv')\n",
    "    USIhs.to_pickle(gen_path + '/hourly/usinas.pkl')\n",
    "wpUSIhs = pd.read_pickle(gen_path + '/hourly/usinas.pkl')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "source": [
    "# load usinas monthly\n",
    "if gen_path + '/monthly/usinas.pkl' not in glob.glob(gen_path + '/monthly/*.pkl'):\n",
    "    USIm = pd.read_csv(gen_path + '/monthly/Comparativo_Geração_de_Energia_Semana_data_usinas.csv',\n",
    "                         sep = ';', index_col = 0, parse_dates = True, dayfirst = True).iloc[1:,[6,8]].sort_index()\n",
    "    # remove missing values\n",
    "    USIm = USIm.loc[USIm.index.notnull()].dropna()\n",
    "    USIm.columns = ['usina','prod_GWh']\n",
    "\n",
    "    USIms = USIm.reset_index().set_index(['usina','Data Escala de Tempo 1 GE Comp 3']).unstack(level=0).prod_GWh\n",
    "    USIms.to_csv(gen_path + '/monthly/usinas.csv')\n",
    "    USIms.to_pickle(gen_path + '/monthly/usinas.pkl')\n",
    "USIms = pd.read_pickle(gen_path + '/monthly/usinas.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "USIhs[USIhs.fillna(0).cumsum(axis=0)==0] = np.nan # remove leading 0s\n",
    "USIhs[USIhs[::-1].fillna(0).cumsum(axis=0)[::-1]==0] = np.nan # remove trailing 0s\n",
    "\n",
    "USIms[USIms.fillna(0).cumsum(axis=0)==0] = np.nan # remove leading 0s\n",
    "USIms[USIms[::-1].fillna(0).cumsum(axis=0)[::-1]==0] = np.nan # remove trailing 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_cap_df(cap,comdate):\n",
    "    com = pd.DataFrame({'capacity': cap}).groupby(comdate).sum()\n",
    "    cap_cum = com.capacity.cumsum()\n",
    "    # if only years given for commissioning dates -> gradual capacity increase over year, full capacity at end of year\n",
    "    if type(cap_cum.index.values[0]) == np.int64:\n",
    "        cap_cum.index = [np.datetime64(str(int(year))+\"-12-31 23:00:00\") for year in cap_cum.index.values]\n",
    "        # create yearly dates at yearends\n",
    "        drcc = pd.date_range(np.datetime64('2005-12-31 23:00:00'),\n",
    "                             np.datetime64('2019-12-31 23:00:00'),freq= 'y')\n",
    "        cap_cum = pd.Series(drcc.map(cap_cum),index = drcc)\n",
    "        # if first year emtpy: either year before or 0 if nothing before\n",
    "        if(sum(com.index<2000) > 0):\n",
    "            cap_cum[0] = com.cumsum()[com.index<2000].max()\n",
    "        else:\n",
    "            cap_cum[0] = 0\n",
    "        # if missing years -> put capacity of year before\n",
    "        cap_cum = cap_cum.ffill()\n",
    "    dr = pd.date_range('1/1/2006','31/12/2019 23:00:00',freq = 'h')\n",
    "    cap_ts = pd.Series(dr.map(cap_cum),index = dr)\n",
    "    cap_ts[0] = cap_cum[cap_cum.index<=pd.Timestamp('2006-01-01')].max()\n",
    "    if type(comdate[0]) == np.int64:\n",
    "        return(cap_ts.interpolate(method='linear'))\n",
    "    else:\n",
    "        return(cap_ts.fillna(method='ffill'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     1,
     10,
     19,
     32,
     53,
     73,
     108
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load and match aneel and ons windparks\n",
    "def matchWords(word, statements):\n",
    "    # function to match a word to different statements\n",
    "    # output: ratio of matching (0-100) for all provided statements\n",
    "    results = []\n",
    "    for s in statements:\n",
    "        r = fuzz.ratio(word, s)\n",
    "        results.append(r)\n",
    "    return results\n",
    "\n",
    "def match_string(string, array):\n",
    "    # function for matching casefolded strings\n",
    "    Slc = string.strip().casefold()\n",
    "    Alc = [arr.casefold() for arr in array.str.strip().unique()]\n",
    "    scores = matchWords(Slc, Alc)\n",
    "    mscore = max(scores)\n",
    "    strarr = array.unique()[np.where(np.array(scores)==mscore)][0]\n",
    "    return(string,strarr,mscore)\n",
    "\n",
    "def match_anl(string):\n",
    "    # function to match ONS to ANL windparks\n",
    "    return(match_string(string,ANL2.name))\n",
    "\n",
    "# load ANEEL and ONS windparks\n",
    "ONS = pd.read_csv(bra_path + '/ONS_windparks.csv', index_col = 0)\n",
    "# remove those with CONJUNTO EOLICO - they're there twice and capacities don't match with ANEEL data\n",
    "ONS = ONS[~ONS.usina.str.contains('CONJUNTO EOLICO')]\n",
    "# remove some other duplicate windparks\n",
    "ONS = ONS[[d not in [' CANOA QUEBRADA (E-RV-ACEP)',' PV DO NORDESTE',' SM (SANTA MARIA)',' SÃO BENTO NORTE II'] for d in ONS.usina]]\n",
    "ANL = pd.read_csv(bra_path + '/turbine_data.csv', index_col = 0)\n",
    "\n",
    "# characters and strings to replace for better matching\n",
    "letters = {'Ãµ':'õ',\n",
    "           'ó':'o',\n",
    "           'ã':'a',\n",
    "           'á':'a',\n",
    "           'â':'a',\n",
    "           'é':'e',\n",
    "           'Ã':'A',\n",
    "           'Á':'A',\n",
    "           'Â':'A',\n",
    "           'Ó':'O',\n",
    "           'É':'E',\n",
    "           'ú':'u',\n",
    "           'ô':'o',\n",
    "           'Ô':'O',\n",
    "           'ú':'u',\n",
    "           'Ú':'U',\n",
    "           'ç':'c',\n",
    "           'Ç':'C',\n",
    "           'í':'i',\n",
    "           'Í':'I',\n",
    "           'Ê':'E'}\n",
    "remove = {' 2LER':'',\n",
    "          ' 2LFA':'',\n",
    "          ' LFA':'',\n",
    "          'EOL ':'',\n",
    "          ' 3LER':'',\n",
    "          'Usina Eolica ':'',\n",
    "          'Eólica ':'',\n",
    "          ' ENERGIAS RENOVAVEIS':'',\n",
    "#          ' CONJUNTO EOLICO':'',\n",
    "          '\\(E-BV-ACEP\\)':'',\n",
    "          '\\(E-RV-ACEP\\)':'',\n",
    "          '\\(BELA BISTA\\)':'',\n",
    "          '\\(ENERGEN\\)':'',\n",
    "          '\\(Antiga Ventos Maranhenses 05\\)':'',\n",
    "          'PARQUE EOLICO ':'',\n",
    "          ' - N HORIZ':'',\n",
    "          'ENERGETICA S/A':'',\n",
    "          '\\(ILHEUS\\)':'',\n",
    "          ' EOLOS':'',\n",
    "          'S\\.A\\.':''}\n",
    "replace = {'LAG DO':'LAGOA DO',\n",
    "           'VENTOS S VICENTE':'VENTOS DE SAO VICENTE',\n",
    "           'SERRA BABILONIA':'SERRA DA BABILONIA',\n",
    "           'CORREDOR SENANDES':'CORREDOR DO SENANDES',\n",
    "           'SAO BENTO NORTE':'SAO BENTO DO NORTE',\n",
    "           'GAMELEIRAS':'GAMELERIAS',\n",
    "           'Lagoinha':'Lagoinh',\n",
    "           'PAPAGAIOS':'PAPAGAIO',\n",
    "           'VENTOS DE SAO ABRAAO':'VENTOS DO SANTO ABRAAO',\n",
    "           'VENTOS DO SAO MARIO':'VENTOS DE SAO MARIO',\n",
    "           'DAGUA':'D AGUA',\n",
    "           'B VEN':'BONS VENTOS',\n",
    "           'NOVA BURITI':'BURITI',\n",
    "           'NOVA CAJUCOCO':'CAJUCOCO',\n",
    "           'PALMAS':'DE PALMAS',\n",
    "           'DE PALMARES':'PALMARES',\n",
    "           'PV DO NORDESTE':'VENTOS DO NORDESTE',\n",
    "           'Aura Lagoa do Barro':'Lagoa do Barro',\n",
    "           'AURA LAGOA DO BARRO':'LAGOA DO BARRO',\n",
    "           'LAGOA BARRO':'LAGOA DO BARRO',\n",
    "           'GRAVATA':'GRAVATA FRUITRADE',\n",
    "           'FAZENDA DO ROSARIO':'FAZENDA ROSARIO',\n",
    "           'Parque Eolico do Horizonte':'Ventos de Horizonte',\n",
    "           'S BENTO':'SAO BENTO',\n",
    "           'SANTO ANTONIO (BTG PACTUAL)':'SANTO ANTONIO DE PADUA',\n",
    "           'SM \\(SANTA MARIA\\)':'SANTA MARIA',\n",
    "           'SAO JORGE CE':'SAO JORGE',\n",
    "           'VENT DA ST ESPERANCA':'VENTOS DA SANTA ESPERANCA',\n",
    "           'VENTOS DA STA DULCE':'VENTOS DA SANTA DULCE',\n",
    "           'ESPERANCA NORDESTE':'ESPERANCA DO NORDESTE',\n",
    "           'Eolica Delta':'Delta',\n",
    "           'Eolica Serra das Vacas':'Serra das Vacas',\n",
    "           'Ventos de Santo Augusto':'Santo Augusto',\n",
    "           'Ventos do Sao Gabriel':'Sao Gabriel',\n",
    "           'GE Maria Helena':'Maria Helena'}\n",
    "numbers = {'10':'X',\n",
    "           '11':'XI',\n",
    "           '12':'XII',\n",
    "           '13':'XIII',\n",
    "           '14':'XIV',\n",
    "           '15':'XV',\n",
    "           '17':'XVII',\n",
    "           '19':'XIX',\n",
    "           '21':'XXI',\n",
    "           '23':'XXIII',\n",
    "           '24':'XXIV',\n",
    "           '25':'XXV',\n",
    "           '26':'XXVI',\n",
    "           '27':'XXVII',\n",
    "           '28':'XXVIII',\n",
    "           '29':'XXIX',\n",
    "           '31':'XXXI',\n",
    "           '34':'XXXIV',\n",
    "           '35':'XXXV',\n",
    "           '36':'XXXVI',\n",
    "           '01':'I',\n",
    "           '02':'II',\n",
    "           '03':'III',\n",
    "           '04':'IV',\n",
    "           '05':'V',\n",
    "           '06':'VI',\n",
    "           '07':'VII',\n",
    "           '08':'VIII',\n",
    "           '09':'IX',\n",
    "           '1':'I',\n",
    "           '2':'II',\n",
    "           '3':'III',\n",
    "           '4':'IV',\n",
    "           '5':'V',\n",
    "           '6':'VI',\n",
    "           '7':'VII',\n",
    "           '8':'VIII',\n",
    "           '9':'IX'}\n",
    "\n",
    "# replace characters\n",
    "ONS2 = ONS.copy(deep=True)\n",
    "ANL2 = ANL.copy(deep=True)\n",
    "for i in letters:\n",
    "    ONS2.usina = ONS2.usina.str.replace(i,letters.get(i))\n",
    "    ANL2.name = ANL2.name.str.replace(i,letters.get(i))\n",
    "for i in replace:\n",
    "    ONS2.usina = ONS2.usina.str.replace(i,replace.get(i))\n",
    "    ANL2.name = ANL2.name.str.replace(i,replace.get(i))\n",
    "for i in remove:\n",
    "    ONS2.usina = ONS2.usina.str.replace(i,remove.get(i))\n",
    "for i in numbers:\n",
    "    ONS2.usina = ONS2.usina.str.replace(i,numbers.get(i))\n",
    "    ANL2.name = ANL2.name.str.replace(i,numbers.get(i))\n",
    "\n",
    "# match windparks\n",
    "matches = ONS2.usina.apply(match_anl).apply(pd.Series)\n",
    "matches.columns = ['ONS_name','ANL_name','score']\n",
    "len(matches[matches.score<100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ONSd = pd.Series(ONS.usina.values,index=ONS2.usina.values)#.reset_index().drop_duplicates()\n",
    "ANLd = pd.Series(ANL.name.values,index=ANL2.name.values)#.reset_index().drop_duplicates()\n",
    "ONSd.columns = ['simpl','orig']\n",
    "ANLd.columns = ['simpl','orig']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get only matching power generation timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches2 = pd.DataFrame({'ANL_name':matches.ANL_name.map(ANLd.drop_duplicates()),\n",
    "                         'ONS_name':matches.ONS_name.map(ONSd),\n",
    "                         'score':matches.score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches2H = matches2.copy(deep=True)\n",
    "matches2H = matches2H[[usi in USIhs.columns.values for usi in matches2.ONS_name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 922,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches2.to_pickle(bra_path + '/matches2.pkl')\n",
    "matches2H.to_pickle(bra_path + '/matches2H.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpUSIm = USIms[matches2.ONS_name[matches2.score==100].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpUSIh = USIhs[matches2H[matches2H.score==100].ONS_name.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract only matching ANEEL data\n",
    "ANLmatch = ANL[[name in matches2.ANL_name[matches2.score==100].values for name in ANL.name]]\n",
    "ANLmatchH = ANL[[name in matches2H.ANL_name[matches2H.score==100].values for name in ANL.name]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load simualted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpERAxr = xr.open_dataset(results_path + '/windpower_stat_ERA5.nc')\n",
    "wpMERxr = xr.open_dataset(results_path + '/windpower_stat_MERRA2.nc')\n",
    "wpERAgxr = xr.open_mfdataset(results_path +'/windpower_??_ERA5_GWA.nc')\n",
    "wpMERgxr = xr.open_mfdataset(results_path +'/windpower_??_MERRA2_GWA.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "turb_mer = pd.read_csv(bra_path + '/turbine_data_mer.csv',index_col=0)\n",
    "turb_era = pd.read_csv(bra_path + '/turbine_data_era.csv',index_col=0)\n",
    "turb_merg = pd.read_csv(bra_path + '/turbine_data_mer_gwa3.csv',index_col=0)\n",
    "turb_erag = pd.read_csv(bra_path + '/turbine_data_era_gwa3.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl = pd.read_csv(bra_path+ '/labels_turbine_data_gwa3.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare simulated data as dataframe\n",
    "wpMERdf = wpMERxr.to_dataframe().unstack().wp\n",
    "wpERAdf = wpERAxr.to_dataframe().unstack().wp\n",
    "wpMERgdf = wpMERgxr.assign_coords(location=range(len(wpMERgxr.location.values))).to_dataframe().unstack().wp\n",
    "wpERAgdf = wpERAgxr.assign_coords(location=range(len(wpERAgxr.location.values))).to_dataframe().unstack().wp\n",
    "# some locations have more than one park, get shares of parks\n",
    "sharesMER = ANL.cap.groupby([lbl.lbl_mer.values,ANL.name.values]).sum()/ANL.cap.groupby([lbl.lbl_mer.values,ANL.name.values]).sum().index.get_level_values(0).map(ANL.cap.groupby(lbl.lbl_mer.values).sum())\n",
    "sharesERA = ANL.cap.groupby([lbl.lbl_era.values,ANL.name.values]).sum()/ANL.cap.groupby([lbl.lbl_era.values,ANL.name.values]).sum().index.get_level_values(0).map(ANL.cap.groupby(lbl.lbl_era.values).sum())\n",
    "sharesMERg = ANL.cap.groupby([lbl.lbl_mer_gwa.values,ANL.name.values]).sum()/ANL.cap.groupby([lbl.lbl_mer_gwa.values,ANL.name.values]).sum().index.get_level_values(0).map(ANL.cap.groupby(lbl.lbl_mer_gwa.values).sum())\n",
    "sharesERAg = ANL.cap.groupby([lbl.lbl_era_gwa.values,ANL.name.values]).sum()/ANL.cap.groupby([lbl.lbl_era_gwa.values,ANL.name.values]).sum().index.get_level_values(0).map(ANL.cap.groupby(lbl.lbl_era_gwa.values).sum())\n",
    "# get generation per park\n",
    "wpMER = wpMERdf.loc[sharesMER.index.codes[0].values()].mul(sharesMER.values,axis=0).groupby(sharesMER.index.get_level_values(1).values).sum().transpose()\n",
    "wpERA = wpERAdf.loc[sharesERA.index.codes[0].values()].mul(sharesERA.values,axis=0).groupby(sharesERA.index.get_level_values(1).values).sum().transpose()\n",
    "wpMERg = wpMERgdf.loc[sharesMERg.index.codes[0].values()].mul(sharesMERg.values,axis=0).groupby(sharesMERg.index.get_level_values(1).values).sum().transpose()\n",
    "wpERAg = wpERAgdf.loc[sharesERAg.index.codes[0].values()].mul(sharesERAg.values,axis=0).groupby(sharesERAg.index.get_level_values(1).values).sum().transpose()\n",
    "# adapt index of MERRA data in 2019 (substract half an hour)\n",
    "wpMER.index = wpMER.index[wpMER.index<'2018-12'].append(wpMER.index[wpMER.index>='2018-12'] - np.timedelta64(30,'m'))\n",
    "wpMERg.index = wpMER.index[wpMERg.index<'2018-12'].append(wpMERg.index[wpMERg.index>='2018-12'] - np.timedelta64(30,'m'))\n",
    "# set time zones\n",
    "wpMER = wpMER.tz_localize('UTC').tz_convert('Etc/GMT-3')\n",
    "wpERA = wpERA.tz_localize('UTC').tz_convert('Etc/GMT-3')\n",
    "wpMERg = wpMERg.tz_localize('UTC').tz_convert('Etc/GMT-3')\n",
    "wpERAg = wpERAg.tz_localize('UTC').tz_convert('Etc/GMT-3')\n",
    "wpUSIh = wpUSIh.tz_localize('Etc/GMT-3')\n",
    "wpUSIm = wpUSIm.tz_localize('Etc/GMT-3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usinas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def analyseUSIh(parks):\n",
    "    compUSIh= pd.DataFrame({'MERRA2':wpMER[parks.ANL_name],\n",
    "                            'ERA5':wpERA[parks.ANL_name],\n",
    "                            'MERRA2_GWA':wpMERg[parks.ANL_name],\n",
    "                            'ERA5_GWA':wpERAg[parks.ANL_name],\n",
    "                            'wp_obs':wpUSIh[parks.ONS_name]*10**6})\n",
    "    # get capacities\n",
    "    capUSIh = get_cap_df(ANL[ANL.name==parks.ANL_name].cap.values,\n",
    "                         ANL[ANL.name==parks.ANL_name].commissioning.astype(np.datetime64).values).tz_localize('UTC').tz_convert('Etc/GMT-3')\n",
    "    # calculate capacity factors\n",
    "    cf_USIh = compUSIh.div(capUSIh,axis=0).dropna()\n",
    "    stat_h = pd.DataFrame({'ERA5':stats(cf_USIh.ERA5,cf_USIh.wp_obs,False),\n",
    "                           'ERA5_GWA':stats(cf_USIh.ERA5_GWA,cf_USIh.wp_obs,False),\n",
    "                           'MERRA2':stats(cf_USIh.MERRA2,cf_USIh.wp_obs,False),\n",
    "                           'MERRA2_GWA':stats(cf_USIh.MERRA2_GWA,cf_USIh.wp_obs,False),\n",
    "                           'obs':[np.nan,np.nan,np.nan,cf_USIh.wp_obs.mean()]},\n",
    "                          index = ['cor','rmse','mbe','avg']).reset_index().melt(id_vars=['index']).dropna()\n",
    "    stat_h.columns = ['param','dataset',parks.ANL_name]\n",
    "    return(stat_h.set_index(['param','dataset']).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def analyseUSId(parks):\n",
    "    compUSIh= pd.DataFrame({'MERRA2':wpMER[parks.ANL_name],\n",
    "                            'ERA5':wpERA[parks.ANL_name],\n",
    "                            'MERRA2_GWA':wpMERg[parks.ANL_name],\n",
    "                            'ERA5_GWA':wpERAg[parks.ANL_name],\n",
    "                            'wp_obs':wpUSIh[parks.ONS_name]*10**6})\n",
    "    # get capacities\n",
    "    capUSIh = get_cap_df(ANL[ANL.name==parks.ANL_name].cap.values,\n",
    "                         ANL[ANL.name==parks.ANL_name].commissioning.astype(np.datetime64).values).tz_localize('UTC').tz_convert('Etc/GMT-3')\n",
    "    ccUSIh = pd.concat([compUSIh,capUSIh],axis=1).dropna()\n",
    "    ccUSIh.columns = compUSIh.columns.tolist() + ['cap']\n",
    "    # aggregate daily\n",
    "    ccUSId = ccUSIh.resample('D').sum()\n",
    "    cf_USId = ccUSId.drop('cap',axis=1).div(ccUSId.cap,axis=0)\n",
    "    stat_d = pd.DataFrame({'ERA5':stats(cf_USId.ERA5,cf_USId.wp_obs,False),\n",
    "                           'ERA5_GWA':stats(cf_USId.ERA5_GWA,cf_USId.wp_obs,False),\n",
    "                           'MERRA2':stats(cf_USId.MERRA2,cf_USId.wp_obs,False),\n",
    "                           'MERRA2_GWA':stats(cf_USId.MERRA2_GWA,cf_USId.wp_obs,False),\n",
    "                           'obs':[np.nan,np.nan,np.nan,cf_USId.wp_obs.mean()]},\n",
    "                          index = ['cor','rmse','mbe','avg']).reset_index().melt(id_vars=['index']).dropna()\n",
    "    stat_d.columns = ['param','dataset',parks.ANL_name]\n",
    "    return(stat_d.set_index(['param','dataset']).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def analyseUSIm(parks):\n",
    "    compUSIh= pd.DataFrame({'MERRA2':wpMER[parks.ANL_name],\n",
    "                            'ERA5':wpERA[parks.ANL_name],\n",
    "                            'MERRA2_GWA':wpMERg[parks.ANL_name],\n",
    "                            'ERA5_GWA':wpERAg[parks.ANL_name],\n",
    "                            'wp_obs':wpUSIm[parks.ONS_name]*10**6}).dropna()\n",
    "    compUSIm = compUSIh.resample('M').sum()\n",
    "    # get capacities\n",
    "    capUSIh = get_cap_df(ANL[ANL.name==parks.ANL_name].cap.values,\n",
    "                         ANL[ANL.name==parks.ANL_name].commissioning.astype(np.datetime64).values).tz_localize('UTC').tz_convert('Etc/GMT-3')\n",
    "    capUSIm = capUSIh.resample('M').sum()\n",
    "    # calculate capacity factors\n",
    "    cf_USIm = compUSIm.div(capUSIm,axis=0).dropna()\n",
    "    stat_m = pd.DataFrame({'ERA5':stats(cf_USIm.ERA5,cf_USIm.wp_obs,False),\n",
    "                           'ERA5_GWA':stats(cf_USIm.ERA5_GWA,cf_USIm.wp_obs,False),\n",
    "                           'MERRA2':stats(cf_USIm.MERRA2,cf_USIm.wp_obs,False),\n",
    "                           'MERRA2_GWA':stats(cf_USIm.MERRA2_GWA,cf_USIm.wp_obs,False),\n",
    "                           'obs':[np.nan,np.nan,np.nan,cf_USIm.wp_obs.mean()]},\n",
    "                          index = ['cor','rmse','mbe','avg']).reset_index().melt(id_vars=['index']).dropna()\n",
    "    stat_m.columns = ['param','dataset',parks.ANL_name]\n",
    "    return(stat_m.set_index(['param','dataset']).transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate stats usinas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kgruber/.conda/envs/py37/lib/python3.7/site-packages/numpy/lib/function_base.py:392: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/home/kgruber/.conda/envs/py37/lib/python3.7/site-packages/numpy/core/_methods.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "/home/kgruber/.conda/envs/py37/lib/python3.7/site-packages/numpy/lib/function_base.py:2522: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  c = cov(x, y, rowvar)\n",
      "/home/kgruber/.conda/envs/py37/lib/python3.7/site-packages/numpy/lib/function_base.py:2451: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "/home/kgruber/.conda/envs/py37/lib/python3.7/site-packages/numpy/lib/function_base.py:2451: RuntimeWarning: invalid value encountered in multiply\n",
      "  c *= np.true_divide(1, fact)\n"
     ]
    }
   ],
   "source": [
    "stats_USIh = pd.concat(matches2H[matches2H.score==100].apply(analyseUSIh,axis=1).tolist(),axis=0).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_USIh.to_csv(results_path + '/stats_USIh.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_USId = pd.concat(matches2H[matches2H.score==100].apply(analyseUSId,axis=1).tolist(),axis=0).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_USId.to_csv(results_path + '/stats_USId.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_USIm = pd.concat(matches2[matches2.score==100].apply(analyseUSIm,axis=1).tolist(),axis=0).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_USIm.to_csv(results_path + '/stats_USIm.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert states in matches dataframes\n",
    "matches2['state'] = matches2.ANL_name.map(ANL.groupby('name').state.first()).values\n",
    "matches2H['state'] = matches2H.ANL_name.map(ANL.groupby('name').state.first()).values\n",
    "# observed generation\n",
    "wpESTh = wpUSIh.groupby(matches2H[matches2H.score==100].state.values,axis=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpESTm = wpUSIm.groupby(matches2[matches2.score==100].state.values,axis=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def getusicapdf(usi):\n",
    "    # function to get the capacity of wind parks neglecting timespans without production data\n",
    "    # usi: ANEEL name of park\n",
    "    c = get_cap_df(ANLmatchH[ANLmatchH.name==usi].cap.values,\n",
    "                   ANLmatchH[ANLmatchH.name==usi].commissioning.astype(np.datetime64).values).tz_localize('UTC').tz_convert('Etc/GMT-3')\n",
    "    mH2 = matches2H[matches2H.score==100]\n",
    "    p = wpUSIh[(mH2.ONS_name[mH2.ANL_name==usi]).values].iloc[:,0]\n",
    "    c[c.index.map(p).isna()] = np.nan\n",
    "    return(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# OLD\n",
    "def analyseESTh(state):\n",
    "    # remove leading and trailing 0s in observed data\n",
    "    wpobs = wpESTh[state].copy(deep=True)\n",
    "    wpobs[wpobs.cumsum(axis=0)==0] = np.nan\n",
    "    wpobs[wpobs[::-1].cumsum(axis=0)[::-1]==0] = np.nan\n",
    "    # get capacities\n",
    "    capusish = pd.Series(ANLmatchH[ANLmatchH.state==state].name.values).apply(getusicapdf).transpose()\n",
    "    capusish.columns = ANLmatchH[ANLmatchH.state==state].name.values\n",
    "    capESTh = capusish.sum(axis=1)\n",
    "    # mask for masking simulated data (to only use timespans where also observed data are available)\n",
    "    mask = capusish.notna()\n",
    "    mask.columns = ANLmatchH[ANLmatchH.state==state].name.values\n",
    "    # mask and aggregate simulated data\n",
    "    wpMER_ESTh = (wpMER[ANLmatchH[ANLmatchH.state==state].name.values]*mask).sum(axis=1)\n",
    "    wpERA_ESTh = (wpERA[ANLmatchH[ANLmatchH.state==state].name.values]*mask).sum(axis=1)\n",
    "    wpMERg_ESTh = (wpMERg[ANLmatchH[ANLmatchH.state==state].name.values]*mask).sum(axis=1)\n",
    "    wpERAg_ESTh = (wpERAg[ANLmatchH[ANLmatchH.state==state].name.values]*mask).sum(axis=1)\n",
    "    compESTh= pd.DataFrame({'MERRA2':wpMER_ESTh,\n",
    "                            'ERA5':wpERA_ESTh,\n",
    "                            'MERRA2_GWA':wpMERg_ESTh,\n",
    "                            'ERA5_GWA':wpERAg_ESTh,\n",
    "                            'wp_obs':wpobs*10**6})\n",
    "    # calculate capacity factors\n",
    "    cf_ESTh = compESTh.div(capESTh,axis=0).dropna()\n",
    "    stat_h = pd.DataFrame({'ERA5':stats(cf_ESTh.ERA5,cf_ESTh.wp_obs,False),\n",
    "                           'ERA5_GWA':stats(cf_ESTh.ERA5_GWA,cf_ESTh.wp_obs,False),\n",
    "                           'MERRA2':stats(cf_ESTh.MERRA2,cf_ESTh.wp_obs,False),\n",
    "                           'MERRA2_GWA':stats(cf_ESTh.MERRA2_GWA,cf_ESTh.wp_obs,False),\n",
    "                           'obs':[np.nan,np.nan,np.nan,cf_ESTh.wp_obs.mean()]},\n",
    "                          index = ['cor','rmse','mbe','avg']).reset_index().melt(id_vars=['index']).dropna()\n",
    "    stat_h.columns = ['param','dataset',state]\n",
    "    return(stat_h.set_index(['param','dataset']).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 889,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def analyseESTh(state):\n",
    "    # remove leading and trailing 0s in observed data\n",
    "    wpobs = wpESTh[state].copy(deep=True)\n",
    "    wpobs[wpobs.cumsum(axis=0)==0] = np.nan\n",
    "    wpobs[wpobs[::-1].cumsum(axis=0)[::-1]==0] = np.nan\n",
    "    # get capacities\n",
    "    capusish = pd.Series(ANLmatchH[ANLmatchH.state==state].name.unique()).apply(getusicapdf).transpose()\n",
    "    capusish.columns = ANLmatchH[ANLmatchH.state==state].name.unique()\n",
    "    capESTh = capusish.sum(axis=1)\n",
    "    # mask for masking simulated data (to only use timespans where also observed data are available)\n",
    "    mask = capusish.notna()\n",
    "    mask.columns = ANLmatchH[ANLmatchH.state==state].name.unique()\n",
    "    # mask and aggregate simulated data\n",
    "    wpMER_ESTh = (wpMER[ANLmatchH[ANLmatchH.state==state].name.unique()]*mask).sum(axis=1)\n",
    "    wpERA_ESTh = (wpERA[ANLmatchH[ANLmatchH.state==state].name.unique()]*mask).sum(axis=1)\n",
    "    wpMERg_ESTh = (wpMERg[ANLmatchH[ANLmatchH.state==state].name.unique()]*mask).sum(axis=1)\n",
    "    wpERAg_ESTh = (wpERAg[ANLmatchH[ANLmatchH.state==state].name.unique()]*mask).sum(axis=1)\n",
    "    compESTh= pd.DataFrame({'MERRA2':wpMER_ESTh,\n",
    "                            'ERA5':wpERA_ESTh,\n",
    "                            'MERRA2_GWA':wpMERg_ESTh,\n",
    "                            'ERA5_GWA':wpERAg_ESTh,\n",
    "                            'wp_obs':wpobs*10**6})\n",
    "    # calculate capacity factors\n",
    "    cf_ESTh = compESTh.div(capESTh,axis=0).dropna()\n",
    "    stat_h = pd.DataFrame({'ERA5':stats(cf_ESTh.ERA5,cf_ESTh.wp_obs,False),\n",
    "                           'ERA5_GWA':stats(cf_ESTh.ERA5_GWA,cf_ESTh.wp_obs,False),\n",
    "                           'MERRA2':stats(cf_ESTh.MERRA2,cf_ESTh.wp_obs,False),\n",
    "                           'MERRA2_GWA':stats(cf_ESTh.MERRA2_GWA,cf_ESTh.wp_obs,False),\n",
    "                           'obs':[np.nan,np.nan,np.nan,cf_ESTh.wp_obs.mean()]},\n",
    "                          index = ['cor','rmse','mbe','avg']).reset_index().melt(id_vars=['index']).dropna()\n",
    "    stat_h.columns = ['param','dataset',state]\n",
    "    return(stat_h.set_index(['param','dataset']).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "#OLD\n",
    "def analyseESTd(state):\n",
    "    # remove leading and trailing 0s in observed data\n",
    "    wpobs = wpESTh[state].copy(deep=True)\n",
    "    wpobs[wpobs.cumsum(axis=0)==0] = np.nan\n",
    "    wpobs[wpobs[::-1].cumsum(axis=0)[::-1]==0] = np.nan\n",
    "    \n",
    "    # get capacities\n",
    "    capusish = pd.Series(ANLmatchH[ANLmatchH.state==state].name.values).apply(getusicapdf).transpose()\n",
    "    capusish.columns = ANLmatchH[ANLmatchH.state==state].name.values\n",
    "    capESTh = capusish.sum(axis=1)\n",
    "    # mask for masking simulated data (to only use timespans where also observed data are available)\n",
    "    mask = capusish.notna()\n",
    "    mask.columns = ANLmatchH[ANLmatchH.state==state].name.values\n",
    "    # mask and aggregate simulated data\n",
    "    wpMER_ESTh = (wpMER[ANLmatchH[ANLmatchH.state==state].name.values]*mask).sum(axis=1)\n",
    "    wpERA_ESTh = (wpERA[ANLmatchH[ANLmatchH.state==state].name.values]*mask).sum(axis=1)\n",
    "    wpMERg_ESTh = (wpMERg[ANLmatchH[ANLmatchH.state==state].name.values]*mask).sum(axis=1)\n",
    "    wpERAg_ESTh = (wpERAg[ANLmatchH[ANLmatchH.state==state].name.values]*mask).sum(axis=1)\n",
    "    compESTh= pd.DataFrame({'MERRA2':wpMER_ESTh,\n",
    "                            'ERA5':wpERA_ESTh,\n",
    "                            'MERRA2_GWA':wpMERg_ESTh,\n",
    "                            'ERA5_GWA':wpERAg_ESTh,\n",
    "                            'wp_obs':wpobs*10**6})\n",
    "    ccESTh = pd.concat([compESTh,capESTh],axis=1).dropna()\n",
    "    ccESTh.columns = compESTh.columns.tolist() + ['cap']\n",
    "    \n",
    "    # aggregate daily\n",
    "    ccESTd = ccESTh.resample('D').sum()\n",
    "    cf_ESTd = ccESTd.drop('cap',axis=1).div(ccESTd.cap,axis=0).dropna()\n",
    "    stat_d = pd.DataFrame({'ERA5':stats(cf_ESTd.ERA5,cf_ESTd.wp_obs,False),\n",
    "                           'ERA5_GWA':stats(cf_ESTd.ERA5_GWA,cf_ESTd.wp_obs,False),\n",
    "                           'MERRA2':stats(cf_ESTd.MERRA2,cf_ESTd.wp_obs,False),\n",
    "                           'MERRA2_GWA':stats(cf_ESTd.MERRA2_GWA,cf_ESTd.wp_obs,False),\n",
    "                           'obs':[np.nan,np.nan,np.nan,cf_ESTd.wp_obs.mean()]},\n",
    "                          index = ['cor','rmse','mbe','avg']).reset_index().melt(id_vars=['index']).dropna()\n",
    "    stat_d.columns = ['param','dataset',state]\n",
    "    return(stat_d.set_index(['param','dataset']).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def analyseESTd(state):\n",
    "    # remove leading and trailing 0s in observed data\n",
    "    wpobs = wpESTh[state].copy(deep=True)\n",
    "    wpobs[wpobs.cumsum(axis=0)==0] = np.nan\n",
    "    wpobs[wpobs[::-1].cumsum(axis=0)[::-1]==0] = np.nan\n",
    "    \n",
    "    # get capacities\n",
    "    capusish = pd.Series(ANLmatchH[ANLmatchH.state==state].name.unique()).apply(getusicapdf).transpose()\n",
    "    capusish.columns = ANLmatchH[ANLmatchH.state==state].name.unique()\n",
    "    capESTh = capusish.sum(axis=1)\n",
    "    # mask for masking simulated data (to only use timespans where also observed data are available)\n",
    "    mask = capusish.notna()\n",
    "    mask.columns = ANLmatchH[ANLmatchH.state==state].name.unique()\n",
    "    # mask and aggregate simulated data\n",
    "    wpMER_ESTh = (wpMER[ANLmatchH[ANLmatchH.state==state].name.unique()]*mask).sum(axis=1)\n",
    "    wpERA_ESTh = (wpERA[ANLmatchH[ANLmatchH.state==state].name.unique()]*mask).sum(axis=1)\n",
    "    wpMERg_ESTh = (wpMERg[ANLmatchH[ANLmatchH.state==state].name.unique()]*mask).sum(axis=1)\n",
    "    wpERAg_ESTh = (wpERAg[ANLmatchH[ANLmatchH.state==state].name.unique()]*mask).sum(axis=1)\n",
    "    compESTh= pd.DataFrame({'MERRA2':wpMER_ESTh,\n",
    "                            'ERA5':wpERA_ESTh,\n",
    "                            'MERRA2_GWA':wpMERg_ESTh,\n",
    "                            'ERA5_GWA':wpERAg_ESTh,\n",
    "                            'wp_obs':wpobs*10**6})\n",
    "    ccESTh = pd.concat([compESTh,capESTh],axis=1).dropna()\n",
    "    ccESTh.columns = compESTh.columns.tolist() + ['cap']\n",
    "    \n",
    "    # aggregate daily\n",
    "    ccESTd = ccESTh.resample('D').sum()\n",
    "    cf_ESTd = ccESTd.drop('cap',axis=1).div(ccESTd.cap,axis=0).dropna()\n",
    "    stat_d = pd.DataFrame({'ERA5':stats(cf_ESTd.ERA5,cf_ESTd.wp_obs,False),\n",
    "                           'ERA5_GWA':stats(cf_ESTd.ERA5_GWA,cf_ESTd.wp_obs,False),\n",
    "                           'MERRA2':stats(cf_ESTd.MERRA2,cf_ESTd.wp_obs,False),\n",
    "                           'MERRA2_GWA':stats(cf_ESTd.MERRA2_GWA,cf_ESTd.wp_obs,False),\n",
    "                           'obs':[np.nan,np.nan,np.nan,cf_ESTd.wp_obs.mean()]},\n",
    "                          index = ['cor','rmse','mbe','avg']).reset_index().melt(id_vars=['index']).dropna()\n",
    "    stat_d.columns = ['param','dataset',state]\n",
    "    return(stat_d.set_index(['param','dataset']).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def getusicapdfM(usi):\n",
    "    # function to get the capacity of wind parks neglecting timespans without production data\n",
    "    # usi: ANEEL name of park\n",
    "    c = get_cap_df(ANLmatch[ANLmatch.name==usi].cap.values,\n",
    "                   ANLmatch[ANLmatch.name==usi].commissioning.astype(np.datetime64).values).tz_localize('UTC').tz_convert('Etc/GMT-3')\n",
    "    m2 = matches2[matches2.score==100]\n",
    "    p = wpUSIm[(m2.ONS_name[m2.ANL_name==usi]).values].iloc[:,0]#.dropna()\n",
    "    cm = c.resample('M').sum()\n",
    "    cym = cm.index.year*100+cm.index.month\n",
    "    pym = pd.Series(p.values,index=p.index.year*100+p.index.month)\n",
    "    cm[cym.map(pym).isna()] = np.nan\n",
    "    return(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# OLD\n",
    "def analyseESTm(state):\n",
    "    print(state)\n",
    "    # remove leading and trailing 0s in observed data\n",
    "    wpobs = wpESTm[state].copy(deep=True)\n",
    "    wpobs[wpobs.cumsum(axis=0)==0] = np.nan\n",
    "    wpobs[wpobs[::-1].cumsum(axis=0)[::-1]==0] = np.nan\n",
    "    # get capacities\n",
    "    capusism = pd.Series(ANLmatch[ANLmatch.state==state].name.values).apply(getusicapdfM).transpose()\n",
    "    capusism.columns = ANLmatch[ANLmatch.state==state].name.values\n",
    "    capESTm = capusism.sum(axis=1)\n",
    "    # mask for masking simulated data (to only use timespans where also observed data are available)\n",
    "    mask = capusism.notna()\n",
    "    mask.columns = ANLmatch[ANLmatch.state==state].name.values\n",
    "    # mask and aggregate simulated data\n",
    "    wpMER_ESTh = (wpMER[ANLmatch[ANLmatch.state==state].name.values]*mask).sum(axis=1)\n",
    "    wpERA_ESTh = (wpERA[ANLmatch[ANLmatch.state==state].name.values]*mask).sum(axis=1)\n",
    "    wpMERg_ESTh = (wpMERg[ANLmatch[ANLmatch.state==state].name.values]*mask).sum(axis=1)\n",
    "    wpERAg_ESTh = (wpERAg[ANLmatch[ANLmatch.state==state].name.values]*mask).sum(axis=1)\n",
    "    compESTh= pd.DataFrame({'MERRA2':wpMER_ESTh,\n",
    "                            'ERA5':wpERA_ESTh,\n",
    "                            'MERRA2_GWA':wpMERg_ESTh,\n",
    "                            'ERA5_GWA':wpERAg_ESTh,\n",
    "                            'wp_obs':wpobs*10**6})\n",
    "    # sum up per month\n",
    "    compESTm = compESTh.resample('M').sum()\n",
    "    # calculate capacity factors\n",
    "    cf_ESTm = compESTm.div(capESTm,axis=0).dropna()\n",
    "    stat_m = pd.DataFrame({'ERA5':stats(cf_ESTm.ERA5,cf_ESTm.wp_obs,False),\n",
    "                           'ERA5_GWA':stats(cf_ESTm.ERA5_GWA,cf_ESTm.wp_obs,False),\n",
    "                           'MERRA2':stats(cf_ESTm.MERRA2,cf_ESTm.wp_obs,False),\n",
    "                           'MERRA2_GWA':stats(cf_ESTm.MERRA2_GWA,cf_ESTm.wp_obs,False),\n",
    "                           'obs':[np.nan,np.nan,np.nan,cf_ESTm.wp_obs.mean()]},\n",
    "                          index = ['cor','rmse','mbe','avg']).reset_index().melt(id_vars=['index']).dropna()\n",
    "    stat_m.columns = ['param','dataset',state]\n",
    "    return(stat_m.set_index(['param','dataset']).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def analyseESTm(state):\n",
    "    print(state)\n",
    "    # remove leading and trailing 0s in observed data\n",
    "    wpobs = wpESTm[state].copy(deep=True)\n",
    "    wpobs[wpobs.cumsum(axis=0)==0] = np.nan\n",
    "    wpobs[wpobs[::-1].cumsum(axis=0)[::-1]==0] = np.nan\n",
    "    # get capacities\n",
    "    capusism = pd.Series(ANLmatch[ANLmatch.state==state].name.unique()).apply(getusicapdfM).transpose()\n",
    "    capusism.columns = ANLmatch[ANLmatch.state==state].name.unique()\n",
    "    capESTm = capusism.sum(axis=1)\n",
    "    # mask for masking simulated data (to only use timespans where also observed data are available)\n",
    "    mask = capusism.notna()\n",
    "    mask.columns = ANLmatch[ANLmatch.state==state].name.unique()\n",
    "    # mask and aggregate simulated data\n",
    "    wpMER_ESTm = (wpMER[ANLmatch[ANLmatch.state==state].name.unique()].resample('M').sum()*mask).sum(axis=1)\n",
    "    wpERA_ESTm = (wpERA[ANLmatch[ANLmatch.state==state].name.unique()].resample('M').sum()*mask).sum(axis=1)\n",
    "    wpMERg_ESTm = (wpMERg[ANLmatch[ANLmatch.state==state].name.unique()].resample('M').sum()*mask).sum(axis=1)\n",
    "    wpERAg_ESTm = (wpERAg[ANLmatch[ANLmatch.state==state].name.unique()].resample('M').sum()*mask).sum(axis=1)\n",
    "    compESTm= pd.DataFrame({'MERRA2':wpMER_ESTm,\n",
    "                            'ERA5':wpERA_ESTm,\n",
    "                            'MERRA2_GWA':wpMERg_ESTm,\n",
    "                            'ERA5_GWA':wpERAg_ESTm,\n",
    "                            'wp_obs':wpobs.resample('M').sum()*10**6})\n",
    "    # calculate capacity factors\n",
    "    cf_ESTm = compESTm.div(capESTm,axis=0).dropna()\n",
    "    stat_m = pd.DataFrame({'ERA5':stats(cf_ESTm.ERA5,cf_ESTm.wp_obs,False),\n",
    "                           'ERA5_GWA':stats(cf_ESTm.ERA5_GWA,cf_ESTm.wp_obs,False),\n",
    "                           'MERRA2':stats(cf_ESTm.MERRA2,cf_ESTm.wp_obs,False),\n",
    "                           'MERRA2_GWA':stats(cf_ESTm.MERRA2_GWA,cf_ESTm.wp_obs,False),\n",
    "                           'obs':[np.nan,np.nan,np.nan,cf_ESTm.wp_obs.mean()]},\n",
    "                          index = ['cor','rmse','mbe','avg']).reset_index().melt(id_vars=['index']).dropna()\n",
    "    stat_m.columns = ['param','dataset',state]\n",
    "    return(stat_m.set_index(['param','dataset']).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_ESTh = pd.concat(pd.Series(matches2H[matches2H.score==100].state.unique()).apply(analyseESTh).tolist(),axis=0).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 893,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_ESTd = pd.concat(pd.Series(matches2H[matches2H.score==100].state.unique()).apply(analyseESTd).tolist(),axis=0).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_ESTh.to_csv(results_path + '/stats_ESTh.csv')\n",
    "stats_ESTd.to_csv(results_path + '/stats_ESTd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PB\n",
      "RN\n",
      "BA\n",
      "SC\n",
      "RS\n",
      "PI\n",
      "SE\n",
      "CE\n",
      "MA\n",
      "PE\n",
      "RJ\n",
      "PR\n"
     ]
    }
   ],
   "source": [
    "stats_ESTm = pd.concat(pd.Series(matches2[matches2.score==100].state.unique()).apply(analyseESTm).tolist(),axis=0).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_ESTm.to_csv(results_path + '/stats_ESTm.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subsystems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "south is the same as RS!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum up northeast\n",
    "wpNEh = wpESTh[['BA','CE','RN']].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum up subsystems\n",
    "subs = {'NE':['BA','CE','MA','PB','PE','PI','RN','SE'],\n",
    "        'S':['PR','RS','SC'],\n",
    "        'SE':['RJ']}\n",
    "wpSUBm = pd.DataFrame({'NE':wpESTm[subs.get('NE')].sum(axis=1),\n",
    "                       'S':wpESTm[subs.get('S')].sum(axis=1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches2H.ONS_name = matches2H.ONS_name.replace(' EOL ANDORINHAS',' TAÍBA ANDORINHA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "code_folding": [
     1,
     34
    ]
   },
   "outputs": [],
   "source": [
    "# OLD\n",
    "def analyseNEh():\n",
    "    # remove leading and trailing 0s in observed data\n",
    "    wpobs = wpNEh.copy(deep=True)\n",
    "    wpobs[wpobs.cumsum(axis=0)==0] = np.nan\n",
    "    wpobs[wpobs[::-1].cumsum(axis=0)[::-1]==0] = np.nan\n",
    "    # get capacities\n",
    "    capusish = pd.Series(ANLmatchH[ANLmatchH.state!='RS'].name.values).apply(getusicapdf).transpose()\n",
    "    capusish.columns = ANLmatchH[ANLmatchH.state!='RS'].name.values\n",
    "    capNEh = capusish.sum(axis=1)\n",
    "    # mask for masking simulated data (to only use timespans where also observed data are available)\n",
    "    mask = capusish.notna()\n",
    "    mask.columns = ANLmatchH[ANLmatchH.state!='RS'].name.values\n",
    "    # mask and aggregate simulated data\n",
    "    wpMER_NEh = (wpMER[ANLmatchH[ANLmatchH.state!='RS'].name.values]*mask).sum(axis=1)\n",
    "    wpERA_NEh = (wpERA[ANLmatchH[ANLmatchH.state!='RS'].name.values]*mask).sum(axis=1)\n",
    "    wpMERg_NEh = (wpMERg[ANLmatchH[ANLmatchH.state!='RS'].name.values]*mask).sum(axis=1)\n",
    "    wpERAg_NEh = (wpERAg[ANLmatchH[ANLmatchH.state!='RS'].name.values]*mask).sum(axis=1)\n",
    "    compNEh= pd.DataFrame({'MERRA2':wpMER_NEh,\n",
    "                            'ERA5':wpERA_NEh,\n",
    "                            'MERRA2_GWA':wpMERg_NEh,\n",
    "                            'ERA5_GWA':wpERAg_NEh,\n",
    "                            'wp_obs':wpobs*10**6})\n",
    "    # calculate capacity factors\n",
    "    cf_NEh = compNEh.div(capNEh,axis=0).dropna()\n",
    "    stat_h = pd.DataFrame({'ERA5':stats(cf_NEh.ERA5,cf_NEh.wp_obs,False),\n",
    "                           'ERA5_GWA':stats(cf_NEh.ERA5_GWA,cf_NEh.wp_obs,False),\n",
    "                           'MERRA2':stats(cf_NEh.MERRA2,cf_NEh.wp_obs,False),\n",
    "                           'MERRA2_GWA':stats(cf_NEh.MERRA2_GWA,cf_NEh.wp_obs,False),\n",
    "                           'obs':[np.nan,np.nan,np.nan,cf_NEh.wp_obs.mean()]},\n",
    "                          index = ['cor','rmse','mbe','avg']).reset_index().melt(id_vars=['index']).dropna()\n",
    "    stat_h.columns = ['param','dataset','NE']\n",
    "    return(stat_h.set_index(['param','dataset']).transpose())\n",
    "# OLDER\n",
    "def analyseNEh():\n",
    "    # remove leading and trailing 0s in observed data\n",
    "    wpobs = wpNEh.copy(deep=True)\n",
    "    wpobs[wpobs.cumsum(axis=0)==0] = np.nan\n",
    "    wpobs[wpobs[::-1].cumsum(axis=0)[::-1]==0] = np.nan\n",
    "    compNEh= pd.DataFrame({'MERRA2':wpMER_NEh,\n",
    "                           'ERA5':wpERA_NEh,\n",
    "                           'MERRA2_GWA':wpMERg_NEh,\n",
    "                           'ERA5_GWA':wpERAg_NEh,\n",
    "                           'wp_obs':wpobs*10**6})\n",
    "    # get capacities\n",
    "    capNEh = get_cap_df(ANLmatchH[[state in ['BA','CE','RN'] for state in ANLmatchH.state]].cap.values,\n",
    "                        ANLmatchH[[state in ['BA','CE','RN'] for state in ANLmatchH.state]].commissioning.astype(np.datetime64).values).tz_localize('UTC').tz_convert('Etc/GMT-3')\n",
    "    # calculate capacity factors\n",
    "    cf_NEh = compNEh.div(capNEh,axis=0).dropna()\n",
    "    stat_h = pd.DataFrame({'ERA5':stats(cf_NEh.ERA5,cf_NEh.wp_obs,False),\n",
    "                           'ERA5_GWA':stats(cf_NEh.ERA5_GWA,cf_NEh.wp_obs,False),\n",
    "                           'MERRA2':stats(cf_NEh.MERRA2,cf_NEh.wp_obs,False),\n",
    "                           'MERRA2_GWA':stats(cf_NEh.MERRA2_GWA,cf_NEh.wp_obs,False),\n",
    "                           'obs':[np.nan,np.nan,np.nan,cf_NEh.wp_obs.mean()]},\n",
    "                          index = ['cor','rmse','mbe','avg']).reset_index().melt(id_vars=['index']).dropna()\n",
    "    stat_h.columns = ['param','dataset','NE']\n",
    "    return(stat_h.set_index(['param','dataset']).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def analyseNEh():\n",
    "    # remove leading and trailing 0s in observed data\n",
    "    wpobs = wpNEh.copy(deep=True)\n",
    "    wpobs[wpobs.cumsum(axis=0)==0] = np.nan\n",
    "    wpobs[wpobs[::-1].cumsum(axis=0)[::-1]==0] = np.nan\n",
    "    # get capacities\n",
    "    capusish = pd.Series(ANLmatchH[ANLmatchH.state!='RS'].name.unique()).apply(getusicapdf).transpose()\n",
    "    capusish.columns = ANLmatchH[ANLmatchH.state!='RS'].name.unique()\n",
    "    capNEh = capusish.sum(axis=1)\n",
    "    # mask for masking simulated data (to only use timespans where also observed data are available)\n",
    "    mask = capusish.notna()\n",
    "    mask.columns = ANLmatchH[ANLmatchH.state!='RS'].name.unique()\n",
    "    # mask and aggregate simulated data\n",
    "    wpMER_NEh = (wpMER[ANLmatchH[ANLmatchH.state!='RS'].name.unique()]*mask).sum(axis=1)\n",
    "    wpERA_NEh = (wpERA[ANLmatchH[ANLmatchH.state!='RS'].name.unique()]*mask).sum(axis=1)\n",
    "    wpMERg_NEh = (wpMERg[ANLmatchH[ANLmatchH.state!='RS'].name.unique()]*mask).sum(axis=1)\n",
    "    wpERAg_NEh = (wpERAg[ANLmatchH[ANLmatchH.state!='RS'].name.unique()]*mask).sum(axis=1)\n",
    "    compNEh= pd.DataFrame({'MERRA2':wpMER_NEh,\n",
    "                            'ERA5':wpERA_NEh,\n",
    "                            'MERRA2_GWA':wpMERg_NEh,\n",
    "                            'ERA5_GWA':wpERAg_NEh,\n",
    "                            'wp_obs':wpobs*10**6})\n",
    "    # calculate capacity factors\n",
    "    cf_NEh = compNEh.div(capNEh,axis=0).dropna()\n",
    "    stat_h = pd.DataFrame({'ERA5':stats(cf_NEh.ERA5,cf_NEh.wp_obs,False),\n",
    "                           'ERA5_GWA':stats(cf_NEh.ERA5_GWA,cf_NEh.wp_obs,False),\n",
    "                           'MERRA2':stats(cf_NEh.MERRA2,cf_NEh.wp_obs,False),\n",
    "                           'MERRA2_GWA':stats(cf_NEh.MERRA2_GWA,cf_NEh.wp_obs,False),\n",
    "                           'obs':[np.nan,np.nan,np.nan,cf_NEh.wp_obs.mean()]},\n",
    "                          index = ['cor','rmse','mbe','avg']).reset_index().melt(id_vars=['index']).dropna()\n",
    "    stat_h.columns = ['param','dataset','NE']\n",
    "    return(stat_h.set_index(['param','dataset']).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "code_folding": [
     1,
     39
    ]
   },
   "outputs": [],
   "source": [
    "# OLD\n",
    "def analyseNEd():\n",
    "    # remove leading and trailing 0s in observed data\n",
    "    wpobs = wpNEh.copy(deep=True)\n",
    "    wpobs[wpobs.cumsum(axis=0)==0] = np.nan\n",
    "    wpobs[wpobs[::-1].cumsum(axis=0)[::-1]==0] = np.nan\n",
    "    \n",
    "    # get capacities\n",
    "    capusish = pd.Series(ANLmatchH[ANLmatchH.state!='RS'].name.values).apply(getusicapdf).transpose()\n",
    "    capusish.columns = ANLmatchH[ANLmatchH.state!='RS'].name.values\n",
    "    capNEh = capusish.sum(axis=1)\n",
    "    # mask for masking simulated data (to only use timespans where also observed data are available)\n",
    "    mask = capusish.notna()\n",
    "    mask.columns = ANLmatchH[ANLmatchH.state!='RS'].name.values\n",
    "    # mask and aggregate simulated data\n",
    "    wpMER_NEh = (wpMER[ANLmatchH[ANLmatchH.state!='RS'].name.values]*mask).sum(axis=1)\n",
    "    wpERA_NEh = (wpERA[ANLmatchH[ANLmatchH.state!='RS'].name.values]*mask).sum(axis=1)\n",
    "    wpMERg_NEh = (wpMERg[ANLmatchH[ANLmatchH.state!='RS'].name.values]*mask).sum(axis=1)\n",
    "    wpERAg_NEh = (wpERAg[ANLmatchH[ANLmatchH.state!='RS'].name.values]*mask).sum(axis=1)\n",
    "    compNEh= pd.DataFrame({'MERRA2':wpMER_NEh,\n",
    "                            'ERA5':wpERA_NEh,\n",
    "                            'MERRA2_GWA':wpMERg_NEh,\n",
    "                            'ERA5_GWA':wpERAg_NEh,\n",
    "                            'wp_obs':wpobs*10**6})\n",
    "    ccNEh = pd.concat([compNEh,capNEh],axis=1).dropna()\n",
    "    ccNEh.columns = compNEh.columns.tolist() + ['cap']\n",
    "    \n",
    "    # aggregate daily\n",
    "    ccNEd = ccNEh.resample('D').sum()\n",
    "    cf_NEd = ccNEd.drop('cap',axis=1).div(ccNEd.cap,axis=0).dropna()\n",
    "    stat_d = pd.DataFrame({'ERA5':stats(cf_NEd.ERA5,cf_NEd.wp_obs,False),\n",
    "                           'ERA5_GWA':stats(cf_NEd.ERA5_GWA,cf_NEd.wp_obs,False),\n",
    "                           'MERRA2':stats(cf_NEd.MERRA2,cf_NEd.wp_obs,False),\n",
    "                           'MERRA2_GWA':stats(cf_NEd.MERRA2_GWA,cf_NEd.wp_obs,False),\n",
    "                           'obs':[np.nan,np.nan,np.nan,cf_NEd.wp_obs.mean()]},\n",
    "                          index = ['cor','rmse','mbe','avg']).reset_index().melt(id_vars=['index']).dropna()\n",
    "    stat_d.columns = ['param','dataset','NE']\n",
    "    return(stat_d.set_index(['param','dataset']).transpose())\n",
    "# OLDER\n",
    "def analyseNEd():\n",
    "    # remove leading and trailing 0s in observed data\n",
    "    wpobs = wpNEh.copy(deep=True)\n",
    "    wpobs[wpobs.cumsum(axis=0)==0] = np.nan\n",
    "    wpobs[wpobs[::-1].cumsum(axis=0)[::-1]==0] = np.nan\n",
    "    compNEh= pd.DataFrame({'MERRA2':wpMER_NEh,\n",
    "                           'ERA5':wpERA_NEh,\n",
    "                           'MERRA2_GWA':wpMERg_NEh,\n",
    "                           'ERA5_GWA':wpERAg_NEh,\n",
    "                           'wp_obs':wpobs*10**6})\n",
    "    # get capacities\n",
    "    capNEh = get_cap_df(ANLmatchH[[state in ['BA','CE','RN'] for state in ANLmatchH.state]].cap.values,\n",
    "                        ANLmatchH[[state in ['BA','CE','RN'] for state in ANLmatchH.state]].commissioning.astype(np.datetime64).values).tz_localize('UTC').tz_convert('Etc/GMT-3')\n",
    "    ccNEh = pd.concat([compNEh,capNEh],axis=1).dropna()\n",
    "    ccNEh.columns = compNEh.columns.tolist() + ['cap']\n",
    "    # aggregate daily\n",
    "    ccNEd = ccNEh.resample('D').sum()\n",
    "    cf_NEd = ccNEd.drop('cap',axis=1).div(ccNEd.cap,axis=0).dropna()\n",
    "    stat_d = pd.DataFrame({'ERA5':stats(cf_NEd.ERA5,cf_NEd.wp_obs,False),\n",
    "                           'ERA5_GWA':stats(cf_NEd.ERA5_GWA,cf_NEd.wp_obs,False),\n",
    "                           'MERRA2':stats(cf_NEd.MERRA2,cf_NEd.wp_obs,False),\n",
    "                           'MERRA2_GWA':stats(cf_NEd.MERRA2_GWA,cf_NEd.wp_obs,False),\n",
    "                           'obs':[np.nan,np.nan,np.nan,cf_NEd.wp_obs.mean()]},\n",
    "                          index = ['cor','rmse','mbe','avg']).reset_index().melt(id_vars=['index']).dropna()\n",
    "    stat_d.columns = ['param','dataset','NE']\n",
    "    return(stat_d.set_index(['param','dataset']).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def analyseNEd():\n",
    "    # remove leading and trailing 0s in observed data\n",
    "    wpobs = wpNEh.copy(deep=True)\n",
    "    wpobs[wpobs.cumsum(axis=0)==0] = np.nan\n",
    "    wpobs[wpobs[::-1].cumsum(axis=0)[::-1]==0] = np.nan\n",
    "    \n",
    "    # get capacities\n",
    "    capusish = pd.Series(ANLmatchH[ANLmatchH.state!='RS'].name.unique()).apply(getusicapdf).transpose()\n",
    "    capusish.columns = ANLmatchH[ANLmatchH.state!='RS'].name.unique()\n",
    "    capNEh = capusish.sum(axis=1)\n",
    "    # mask for masking simulated data (to only use timespans where also observed data are available)\n",
    "    mask = capusish.notna()\n",
    "    mask.columns = ANLmatchH[ANLmatchH.state!='RS'].name.unique()\n",
    "    # mask and aggregate simulated data\n",
    "    wpMER_NEh = (wpMER[ANLmatchH[ANLmatchH.state!='RS'].name.unique()]*mask).sum(axis=1)\n",
    "    wpERA_NEh = (wpERA[ANLmatchH[ANLmatchH.state!='RS'].name.unique()]*mask).sum(axis=1)\n",
    "    wpMERg_NEh = (wpMERg[ANLmatchH[ANLmatchH.state!='RS'].name.unique()]*mask).sum(axis=1)\n",
    "    wpERAg_NEh = (wpERAg[ANLmatchH[ANLmatchH.state!='RS'].name.unique()]*mask).sum(axis=1)\n",
    "    compNEh= pd.DataFrame({'MERRA2':wpMER_NEh,\n",
    "                            'ERA5':wpERA_NEh,\n",
    "                            'MERRA2_GWA':wpMERg_NEh,\n",
    "                            'ERA5_GWA':wpERAg_NEh,\n",
    "                            'wp_obs':wpobs*10**6})\n",
    "    ccNEh = pd.concat([compNEh,capNEh],axis=1).dropna()\n",
    "    ccNEh.columns = compNEh.columns.tolist() + ['cap']\n",
    "    \n",
    "    # aggregate daily\n",
    "    ccNEd = ccNEh.resample('D').sum()\n",
    "    cf_NEd = ccNEd.drop('cap',axis=1).div(ccNEd.cap,axis=0).dropna()\n",
    "    stat_d = pd.DataFrame({'ERA5':stats(cf_NEd.ERA5,cf_NEd.wp_obs,False),\n",
    "                           'ERA5_GWA':stats(cf_NEd.ERA5_GWA,cf_NEd.wp_obs,False),\n",
    "                           'MERRA2':stats(cf_NEd.MERRA2,cf_NEd.wp_obs,False),\n",
    "                           'MERRA2_GWA':stats(cf_NEd.MERRA2_GWA,cf_NEd.wp_obs,False),\n",
    "                           'obs':[np.nan,np.nan,np.nan,cf_NEd.wp_obs.mean()]},\n",
    "                          index = ['cor','rmse','mbe','avg']).reset_index().melt(id_vars=['index']).dropna()\n",
    "    stat_d.columns = ['param','dataset','NE']\n",
    "    return(stat_d.set_index(['param','dataset']).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def analyseSUBm(SUB):\n",
    "    print(SUB)\n",
    "    # remove leading and trailing 0s in observed data\n",
    "    wpobs = wpSUBm[SUB].copy(deep=True)\n",
    "    wpobs[wpobs.cumsum(axis=0)==0] = np.nan\n",
    "    wpobs[wpobs[::-1].cumsum(axis=0)[::-1]==0] = np.nan\n",
    "    # get ANEEL data for subsystem\n",
    "    ANLmatchSUB = ANLmatch[[state in subs.get(SUB) for state in ANLmatch.state]]\n",
    "    # get capacities\n",
    "    capusism = pd.Series(ANLmatchSUB.name.unique()).apply(getusicapdfM).transpose()\n",
    "    capusism.columns = ANLmatchSUB.name.unique()\n",
    "    capSUBm = capusism.sum(axis=1)\n",
    "    # mask for masking simulated data (to only use timespans where also observed data are available)\n",
    "    mask = capusism.notna()\n",
    "    mask.columns = ANLmatchSUB.name.unique()\n",
    "    # mask and aggregate simulated data\n",
    "    wpMER_SUBm = (wpMER[ANLmatchSUB.name.unique()].resample('M').sum()*mask).sum(axis=1)\n",
    "    wpERA_SUBm = (wpERA[ANLmatchSUB.name.unique()].resample('M').sum()*mask).sum(axis=1)\n",
    "    wpMERg_SUBm = (wpMERg[ANLmatchSUB.name.unique()].resample('M').sum()*mask).sum(axis=1)\n",
    "    wpERAg_SUBm = (wpERAg[ANLmatchSUB.name.unique()].resample('M').sum()*mask).sum(axis=1)\n",
    "    compSUBm= pd.DataFrame({'MERRA2':wpMER_SUBm,\n",
    "                            'ERA5':wpERA_SUBm,\n",
    "                            'MERRA2_GWA':wpMERg_SUBm,\n",
    "                            'ERA5_GWA':wpERAg_SUBm,\n",
    "                            'wp_obs':wpobs.resample('M').sum()*10**6})\n",
    "    # calculate capacity factors\n",
    "    cf_SUBm = compSUBm.div(capSUBm,axis=0).dropna()\n",
    "    stat_m = pd.DataFrame({'ERA5':stats(cf_SUBm.ERA5,cf_SUBm.wp_obs,False),\n",
    "                           'ERA5_GWA':stats(cf_SUBm.ERA5_GWA,cf_SUBm.wp_obs,False),\n",
    "                           'MERRA2':stats(cf_SUBm.MERRA2,cf_SUBm.wp_obs,False),\n",
    "                           'MERRA2_GWA':stats(cf_SUBm.MERRA2_GWA,cf_SUBm.wp_obs,False),\n",
    "                           'obs':[np.nan,np.nan,np.nan,cf_SUBm.wp_obs.mean()]},\n",
    "                          index = ['cor','rmse','mbe','avg']).reset_index().melt(id_vars=['index']).dropna()\n",
    "    stat_m.columns = ['param','dataset',SUB]\n",
    "    return(stat_m.set_index(['param','dataset']).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_NEh = analyseNEh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_NEh.to_csv(results_path + '/stats_SUBh.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_NEd = analyseNEd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_NEd.to_csv(results_path + '/stats_SUBd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NE\n",
      "S\n"
     ]
    }
   ],
   "source": [
    "stats_SUBm = pd.concat(pd.Series(['NE','S']).apply(analyseSUBm).tolist(),axis=0).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_SUBm.to_csv(results_path + '/stats_SUBm.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brazil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum up brazil\n",
    "wpMER_BRAh = wpMER_ESTh.sum(axis=1)\n",
    "wpERA_BRAh = wpERA_ESTh.sum(axis=1)\n",
    "wpMERg_BRAh = wpMERg_ESTh.sum(axis=1)\n",
    "wpERAg_BRAh = wpERAg_ESTh.sum(axis=1)\n",
    "wpBRAh = wpESTh.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 901,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpBRAm = wpESTm.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 916,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def analyseBRAh():\n",
    "    # remove leading and trailing 0s in observed data\n",
    "    wpobs = wpBRAh.copy(deep=True)\n",
    "    wpobs[wpobs.cumsum(axis=0)==0] = np.nan\n",
    "    wpobs[wpobs[::-1].cumsum(axis=0)[::-1]==0] = np.nan\n",
    "    # get capacities\n",
    "    capusish = pd.Series(ANLmatchH.name.unique()).apply(getusicapdf).transpose()\n",
    "    capusish.columns = ANLmatchH.name.unique()\n",
    "    capBRAh = capusish.sum(axis=1)\n",
    "    # mask for masking simulated data (to only use timespans where also observed data are available)\n",
    "    mask = capusish.notna()\n",
    "    mask.columns = ANLmatchH.name.unique()\n",
    "    # mask and aggregate simulated data\n",
    "    wpMER_BRAh = (wpMER*mask).sum(axis=1)\n",
    "    wpERA_BRAh = (wpERA*mask).sum(axis=1)\n",
    "    wpMERg_BRAh = (wpMERg*mask).sum(axis=1)\n",
    "    wpERAg_BRAh = (wpERAg*mask).sum(axis=1)\n",
    "    compBRAh= pd.DataFrame({'MERRA2':wpMER_BRAh,\n",
    "                            'ERA5':wpERA_BRAh,\n",
    "                            'MERRA2_GWA':wpMERg_BRAh,\n",
    "                            'ERA5_GWA':wpERAg_BRAh,\n",
    "                            'wp_obs':wpobs*10**6})\n",
    "    # calculate capacity factors\n",
    "    cf_BRAh = compBRAh.div(capBRAh,axis=0).dropna()\n",
    "    stat_h = pd.DataFrame({'ERA5':stats(cf_BRAh.ERA5,cf_BRAh.wp_obs,False),\n",
    "                           'ERA5_GWA':stats(cf_BRAh.ERA5_GWA,cf_BRAh.wp_obs,False),\n",
    "                           'MERRA2':stats(cf_BRAh.MERRA2,cf_BRAh.wp_obs,False),\n",
    "                           'MERRA2_GWA':stats(cf_BRAh.MERRA2_GWA,cf_BRAh.wp_obs,False),\n",
    "                           'obs':[np.nan,np.nan,np.nan,cf_BRAh.wp_obs.mean()]},\n",
    "                          index = ['cor','rmse','mbe','avg']).reset_index().melt(id_vars=['index']).dropna()\n",
    "    stat_h.columns = ['param','dataset','BRA']\n",
    "    return(stat_h.set_index(['param','dataset']).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 912,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def analyseBRAd():\n",
    "    # remove leading and trailing 0s in observed data\n",
    "    wpobs = wpBRAh.copy(deep=True)\n",
    "    wpobs[wpobs.cumsum(axis=0)==0] = np.nan\n",
    "    wpobs[wpobs[::-1].cumsum(axis=0)[::-1]==0] = np.nan\n",
    "    \n",
    "    # get capacities\n",
    "    capusish = pd.Series(ANLmatchH.name.unique()).apply(getusicapdf).transpose()\n",
    "    capusish.columns = ANLmatchH.name.unique()\n",
    "    capBRAh = capusish.sum(axis=1)\n",
    "    # mask for masking simulated data (to only use timespans where also observed data are available)\n",
    "    mask = capusish.notna()\n",
    "    mask.columns = ANLmatchH.name.unique()\n",
    "    # mask and aggregate simulated data\n",
    "    wpMER_BRAh = (wpMER*mask).sum(axis=1)\n",
    "    wpERA_BRAh = (wpERA*mask).sum(axis=1)\n",
    "    wpMERg_BRAh = (wpMERg*mask).sum(axis=1)\n",
    "    wpERAg_BRAh = (wpERAg*mask).sum(axis=1)\n",
    "    compBRAh= pd.DataFrame({'MERRA2':wpMER_BRAh,\n",
    "                            'ERA5':wpERA_BRAh,\n",
    "                            'MERRA2_GWA':wpMERg_BRAh,\n",
    "                            'ERA5_GWA':wpERAg_BRAh,\n",
    "                            'wp_obs':wpobs*10**6})\n",
    "    ccBRAh = pd.concat([compBRAh,capBRAh],axis=1).dropna()\n",
    "    ccBRAh.columns = compBRAh.columns.tolist() + ['cap']\n",
    "    \n",
    "    # aggregate daily\n",
    "    ccBRAd = ccBRAh.resample('D').sum()\n",
    "    cf_BRAd = ccBRAd.drop('cap',axis=1).div(ccBRAd.cap,axis=0).dropna()\n",
    "    stat_d = pd.DataFrame({'ERA5':stats(cf_BRAd.ERA5,cf_BRAd.wp_obs,False),\n",
    "                           'ERA5_GWA':stats(cf_BRAd.ERA5_GWA,cf_BRAd.wp_obs,False),\n",
    "                           'MERRA2':stats(cf_BRAd.MERRA2,cf_BRAd.wp_obs,False),\n",
    "                           'MERRA2_GWA':stats(cf_BRAd.MERRA2_GWA,cf_BRAd.wp_obs,False),\n",
    "                           'obs':[np.nan,np.nan,np.nan,cf_BRAd.wp_obs.mean()]},\n",
    "                          index = ['cor','rmse','mbe','avg']).reset_index().melt(id_vars=['index']).dropna()\n",
    "    stat_d.columns = ['param','dataset','BRA']\n",
    "    return(stat_d.set_index(['param','dataset']).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# OLD\n",
    "\n",
    "def analyseBRAh():\n",
    "    # remove leading and trailing 0s in observed data\n",
    "    wpobs = wpBRAh.copy(deep=True)\n",
    "    wpobs[wpobs.cumsum(axis=0)==0] = np.nan\n",
    "    wpobs[wpobs[::-1].cumsum(axis=0)[::-1]==0] = np.nan\n",
    "    compBRAh= pd.DataFrame({'MERRA2':wpMER_BRAh,\n",
    "                            'ERA5':wpERA_BRAh,\n",
    "                            'MERRA2_GWA':wpMERg_BRAh,\n",
    "                            'ERA5_GWA':wpERAg_BRAh,\n",
    "                            'wp_obs':wpobs*10**6})\n",
    "    # get capacities\n",
    "    capBRAh = get_cap_df(ANLmatchH.cap.values,\n",
    "                         ANLmatchH.commissioning.astype(np.datetime64).values).tz_localize('UTC').tz_convert('Etc/GMT-3')\n",
    "    # calculate capacity factors\n",
    "    cf_BRAh = compBRAh.div(capBRAh,axis=0).dropna()\n",
    "    stat_h = pd.DataFrame({'ERA5':stats(cf_BRAh.ERA5,cf_BRAh.wp_obs,False),\n",
    "                           'ERA5_GWA':stats(cf_BRAh.ERA5_GWA,cf_BRAh.wp_obs,False),\n",
    "                           'MERRA2':stats(cf_BRAh.MERRA2,cf_BRAh.wp_obs,False),\n",
    "                           'MERRA2_GWA':stats(cf_BRAh.MERRA2_GWA,cf_BRAh.wp_obs,False),\n",
    "                           'obs':[np.nan,np.nan,np.nan,cf_BRAh.wp_obs.mean()]},\n",
    "                          index = ['cor','rmse','mbe','avg']).reset_index().melt(id_vars=['index']).dropna()\n",
    "    stat_h.columns = ['param','dataset','BRA']\n",
    "    return(stat_h.set_index(['param','dataset']).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# OLD\n",
    "\n",
    "def analyseBRAd():\n",
    "    # remove leading and trailing 0s in observed data\n",
    "    wpobs = wpBRAh.copy(deep=True)\n",
    "    wpobs[wpobs.cumsum(axis=0)==0] = np.nan\n",
    "    wpobs[wpobs[::-1].cumsum(axis=0)[::-1]==0] = np.nan\n",
    "    compBRAh= pd.DataFrame({'MERRA2':wpMER_BRAh,\n",
    "                           'ERA5':wpERA_BRAh,\n",
    "                           'MERRA2_GWA':wpMERg_BRAh,\n",
    "                           'ERA5_GWA':wpERAg_BRAh,\n",
    "                           'wp_obs':wpobs*10**6})\n",
    "    # get capacities\n",
    "    capBRAh = get_cap_df(ANLmatchH.cap.values,\n",
    "                         ANLmatchH.commissioning.astype(np.datetime64).values).tz_localize('UTC').tz_convert('Etc/GMT-3')\n",
    "    ccBRAh = pd.concat([compBRAh,capBRAh],axis=1).dropna()\n",
    "    ccBRAh.columns = compBRAh.columns.tolist() + ['cap']\n",
    "    # aggregate daily\n",
    "    ccBRAd = ccBRAh.resample('D').sum()\n",
    "    cf_BRAd = ccBRAd.drop('cap',axis=1).div(ccBRAd.cap,axis=0).dropna()\n",
    "    stat_d = pd.DataFrame({'ERA5':stats(cf_BRAd.ERA5,cf_BRAd.wp_obs,False),\n",
    "                           'ERA5_GWA':stats(cf_BRAd.ERA5_GWA,cf_BRAd.wp_obs,False),\n",
    "                           'MERRA2':stats(cf_BRAd.MERRA2,cf_BRAd.wp_obs,False),\n",
    "                           'MERRA2_GWA':stats(cf_BRAd.MERRA2_GWA,cf_BRAd.wp_obs,False),\n",
    "                           'obs':[np.nan,np.nan,np.nan,cf_BRAd.wp_obs.mean()]},\n",
    "                          index = ['cor','rmse','mbe','avg']).reset_index().melt(id_vars=['index']).dropna()\n",
    "    stat_d.columns = ['param','dataset','BRA']\n",
    "    return(stat_d.set_index(['param','dataset']).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def analyseBRAm():\n",
    "    # remove leading and trailing 0s in observed data\n",
    "    wpobs = wpBRAm.copy(deep=True)\n",
    "    wpobs[wpobs.cumsum(axis=0)==0] = np.nan\n",
    "    wpobs[wpobs[::-1].cumsum(axis=0)[::-1]==0] = np.nan\n",
    "    # get capacities\n",
    "    capusism = pd.Series(ANLmatch.name.unique()).apply(getusicapdfM).transpose()\n",
    "    capusism.columns = ANLmatch.name.unique()\n",
    "    capBRAm = capusism.sum(axis=1)\n",
    "    # mask for masking simulated data (to only use timespans where also observed data are available)\n",
    "    mask = capusism.notna()\n",
    "    mask.columns = ANLmatch.name.unique()\n",
    "    # mask and aggregate simulated data\n",
    "    wpMER_BRAm = (wpMER[ANLmatch.name.unique()].resample('M').sum()*mask).sum(axis=1)\n",
    "    wpERA_BRAm = (wpERA[ANLmatch.name.unique()].resample('M').sum()*mask).sum(axis=1)\n",
    "    wpMERg_BRAm = (wpMERg[ANLmatch.name.unique()].resample('M').sum()*mask).sum(axis=1)\n",
    "    wpERAg_BRAm = (wpERAg[ANLmatch.name.unique()].resample('M').sum()*mask).sum(axis=1)\n",
    "    compBRAm= pd.DataFrame({'MERRA2':wpMER_BRAm,\n",
    "                            'ERA5':wpERA_BRAm,\n",
    "                            'MERRA2_GWA':wpMERg_BRAm,\n",
    "                            'ERA5_GWA':wpERAg_BRAm,\n",
    "                            'wp_obs':wpobs.resample('M').sum()*10**6})\n",
    "    # calculate capacity factors\n",
    "    cf_BRAm = compBRAm.div(capBRAm,axis=0).dropna()\n",
    "    stat_m = pd.DataFrame({'ERA5':stats(cf_BRAm.ERA5,cf_BRAm.wp_obs,False),\n",
    "                           'ERA5_GWA':stats(cf_BRAm.ERA5_GWA,cf_BRAm.wp_obs,False),\n",
    "                           'MERRA2':stats(cf_BRAm.MERRA2,cf_BRAm.wp_obs,False),\n",
    "                           'MERRA2_GWA':stats(cf_BRAm.MERRA2_GWA,cf_BRAm.wp_obs,False),\n",
    "                           'obs':[np.nan,np.nan,np.nan,cf_BRAm.wp_obs.mean()]},\n",
    "                          index = ['cor','rmse','mbe','avg']).reset_index().melt(id_vars=['index']).dropna()\n",
    "    stat_m.columns = ['param','dataset','BRA']\n",
    "    return(stat_m.set_index(['param','dataset']).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 917,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_BRAh = analyseBRAh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 919,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_BRAh.to_csv(results_path + '/stats_BRAh.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 913,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_BRAd = analyseBRAd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 915,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_BRAd.to_csv(results_path + '/stats_BRAd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 907,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_BRAm = analyseBRAm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 909,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_BRAm.to_csv(results_path + '/stats_BRAm.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-py37]",
   "language": "python",
   "name": "conda-env-.conda-py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
