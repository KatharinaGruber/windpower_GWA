{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from paths_bra import *\n",
    "\n",
    "from dask.diagnostics import ProgressBar\n",
    "ProgressBar().register()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare turbine location data\n"
     ]
    }
   ],
   "source": [
    "# MERRA-2 and ERA5 only unique interpolated locations\n",
    "print('prepare turbine location data')\n",
    "# open turbine files\n",
    "wt_mer = pd.read_csv(bra_path + '/turbine_data_mer.csv', index_col=0)\n",
    "wt_era = pd.read_csv(bra_path + '/turbine_data_era.csv', index_col=0)\n",
    "\n",
    "# open wind files\n",
    "wind_mer = xr.open_mfdataset(mer_path + \"/eff_ws/merra2_wind_BRA_*.nc\", chunks = {'time': 38})\n",
    "alpha_mer = xr.open_mfdataset(mer_path + \"/eff_ws/merra2_alpha_BRA_*.nc\", chunks = {'time': 38})\n",
    "wind_era = xr.open_mfdataset(era_path + \"/eff_ws/era5_wind_BRA_*.nc\", chunks = {'time': 38})\n",
    "alpha_era = xr.open_mfdataset(era_path + \"/eff_ws/era5_alpha_BRA_*.nc\", chunks = {'time': 38})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n"
     ]
    }
   ],
   "source": [
    "# Create dataframe with sequence the size of MERRA-2 grid to find out which turbines interpolate to the same point\n",
    "in_seq_mer = xr.Dataset({'x':(['lat','lon'],\n",
    "                              np.array(range(wind_mer.wh50.isel(time=0).values.size)).reshape(wind_mer.wh50.isel(time=0).values.shape))},\n",
    "                         coords = {'lat':wind_mer.lat.values,\n",
    "                                   'lon':wind_mer.lon.values})\n",
    "in_seq_era = xr.Dataset({'x':(['lat','lon'],\n",
    "                              np.array(range(wind_era.wh100.isel(time=0).values.size)).reshape(wind_era.wh100.isel(time=0).values.shape))},\n",
    "                         coords = {'lat':wind_era.latitude.values,\n",
    "                                   'lon':wind_era.longitude.values})\n",
    "\n",
    "# interpolate to reanalysis grid points\n",
    "ip_mer = in_seq_mer.interp(coords={\"lon\":xr.DataArray(wt_mer.lon,dims='location'),\n",
    "                                   \"lat\":xr.DataArray(wt_mer.lat,dims='location')},method=\"nearest\").to_dataframe()\n",
    "ip_era = in_seq_era.interp(coords={\"lon\":xr.DataArray(wt_era.lon,dims='location'),\n",
    "                                   \"lat\":xr.DataArray(wt_era.lat,dims='location')},method=\"nearest\").to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find unique locations\n",
    "uniques_mer = ip_mer.groupby(ip_mer.x).min()\n",
    "uniques_era = ip_era.groupby(ip_era.x).min()\n",
    "\n",
    "# add ids to unique correlation locations\n",
    "uniques_era['cor_id'] = range(len(uniques_era.index))\n",
    "uniques_mer['cor_id'] = range(len(uniques_mer.index))\n",
    "\n",
    "# add correlation ids to wind turbine data\n",
    "wt_mer['cor_id'] = ip_mer.x.map(uniques_mer.cor_id)\n",
    "wt_era['cor_id'] = ip_era.x.map(uniques_era.cor_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANL = pd.read_csv(bra_path + '/turbine_data.csv', index_col = 0)\n",
    "lbl = pd.read_csv(bra_path+ '/labels_turbine_data_gwa3.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# usinas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some locations have more than one park, get shares of parks\n",
    "sharesMER = ANL.cap.groupby([lbl.lbl_mer.values,ANL.name.values]).sum()/ANL.cap.groupby([lbl.lbl_mer.values,ANL.name.values]).sum().index.get_level_values(0).map(ANL.cap.groupby(lbl.lbl_mer.values).sum())\n",
    "sharesERA = ANL.cap.groupby([lbl.lbl_era.values,ANL.name.values]).sum()/ANL.cap.groupby([lbl.lbl_era.values,ANL.name.values]).sum().index.get_level_values(0).map(ANL.cap.groupby(lbl.lbl_era.values).sum())\n",
    "sharesMERg = ANL.cap.groupby([lbl.lbl_mer_gwa.values,ANL.name.values]).sum()/ANL.cap.groupby([lbl.lbl_mer_gwa.values,ANL.name.values]).sum().index.get_level_values(0).map(ANL.cap.groupby(lbl.lbl_mer_gwa.values).sum())\n",
    "sharesERAg = ANL.cap.groupby([lbl.lbl_era_gwa.values,ANL.name.values]).sum()/ANL.cap.groupby([lbl.lbl_era_gwa.values,ANL.name.values]).sum().index.get_level_values(0).map(ANL.cap.groupby(lbl.lbl_era_gwa.values).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add correlation ids to shares\n",
    "sharesMER = pd.DataFrame({'share':sharesMER,\n",
    "                          'cor_id':sharesMER.index.get_level_values(0).map(pd.Series(wt_mer.cor_id.values,index=lbl.lbl_mer.unique()))})\n",
    "sharesERA = pd.DataFrame({'share':sharesERA,\n",
    "                          'cor_id':sharesERA.index.get_level_values(0).map(pd.Series(wt_era.cor_id.values,index=lbl.lbl_era.unique()))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group ids by park\n",
    "cidMER = sharesMER.groupby(sharesMER.index.get_level_values(1)).cor_id.unique()\n",
    "cidERA = sharesERA.groupby(sharesERA.index.get_level_values(1)).cor_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of grid cells\n",
    "ngc_USI_mer = cidMER.apply(len)\n",
    "ngc_USI_era = cidERA.apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load matching parks\n",
    "mpM = pd.read_pickle(bra_path + '/matches2.pkl')\n",
    "mpH = pd.read_pickle(bra_path + '/matches2H.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngc_EST_mer = wt_mer.groupby('state').cor_id.unique().apply(len)\n",
    "ngc_EST_era = wt_era.groupby('state').cor_id.unique().apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get correlation ids per park found in hourly and monthly data\n",
    "cid_usi_merM = pd.Series(mpM.ANL_name[mpM.score==100].map(cidMER).values,index = mpM.ANL_name[mpM.score==100])\n",
    "cid_usi_eraM = pd.Series(mpM.ANL_name[mpM.score==100].map(cidERA).values,index = mpM.ANL_name[mpM.score==100])\n",
    "cid_usi_merH = pd.Series(mpH.ANL_name[mpH.score==100].map(cidMER).values,index = mpH.ANL_name[mpH.score==100])\n",
    "cid_usi_eraH = pd.Series(mpH.ANL_name[mpH.score==100].map(cidERA).values,index = mpH.ANL_name[mpH.score==100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "usiest = ANL.groupby('name').state.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngc_EST_merM = cid_usi_merM.groupby(cid_usi_merM.index.map(usiest)).apply(np.concatenate).apply(np.unique).apply(len)\n",
    "ngc_EST_eraM = cid_usi_eraM.groupby(cid_usi_eraM.index.map(usiest)).apply(np.concatenate).apply(np.unique).apply(len)\n",
    "ngc_EST_merH = cid_usi_merH.groupby(cid_usi_merH.index.map(usiest)).apply(np.concatenate).apply(np.unique).apply(len)\n",
    "ngc_EST_eraH = cid_usi_eraH.groupby(cid_usi_eraH.index.map(usiest)).apply(np.concatenate).apply(np.unique).apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# subsystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "subH = pd.Series(['NE']*3,index=['BA','CE','RN'])\n",
    "subM = pd.Series(['NE']*8+['S']*3,index=['BA','CE','MA','PB','PE','PI','RN','SE','PR','RS','SC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngc_SUB_merM = cid_usi_merM.groupby(cid_usi_merM.index.map(usiest).map(subM)).apply(np.concatenate).apply(np.unique).apply(len)\n",
    "ngc_SUB_eraM = cid_usi_eraM.groupby(cid_usi_eraM.index.map(usiest).map(subM)).apply(np.concatenate).apply(np.unique).apply(len)\n",
    "ngc_SUB_merH = cid_usi_merH.groupby(cid_usi_merH.index.map(usiest).map(subH)).apply(np.concatenate).apply(np.unique).apply(len)\n",
    "ngc_SUB_eraH = cid_usi_eraH.groupby(cid_usi_eraH.index.map(usiest).map(subH)).apply(np.concatenate).apply(np.unique).apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brazil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngc_BRA_mer = uniques_mer.shape[0]\n",
    "ngc_BRA_era = uniques_era.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge all sizes\n"
     ]
    }
   ],
   "source": [
    "# merge sizes\n",
    "print('merge all sizes')\n",
    "ngc = pd.DataFrame({'scale': ['country']*6 + \n",
    "                                   ['state']*2*(2*len(ngc_EST_merH)+len(ngc_EST_merM)) + \n",
    "                                    ['subsystem']*2*(2*len(ngc_SUB_merH)+len(ngc_SUB_merM)) + \n",
    "                                    ['park']*6*len(ngc_USI_mer),\n",
    "                          'region': ['BRA',]*6 + \n",
    "                                    ngc_EST_merH.index.to_list()*4 + ngc_EST_merM.index.to_list()*2 +\n",
    "                                    ['NE']*4 + ['NE','S']*2 + \n",
    "                                    ngc_USI_mer.index.to_list()*6,\n",
    "                          'dataset': np.repeat(['MERRA2','ERA5'],3).tolist() + \n",
    "                                     np.repeat(['MERRA2','ERA5'],len(ngc_EST_merH)).tolist()*2 + np.repeat(['MERRA2','ERA5'],len(ngc_EST_merM)).tolist()+ \n",
    "                                     np.repeat(['MERRA2','ERA5'],len(ngc_SUB_merH)).tolist()*2 + np.repeat(['MERRA2','ERA5'],len(ngc_SUB_merM)).tolist() +\n",
    "                                     np.repeat(['MERRA2','ERA5'],len(ngc_USI_mer)).tolist()*3,\n",
    "                          'temp':['m','d','h']*2+\n",
    "                                 np.repeat(['h','d'],2*len(ngc_EST_merH)).tolist()+['m']*2*len(ngc_EST_merM)+\n",
    "                                 np.repeat(['h','d'],2*len(ngc_SUB_merH)).tolist()+['m']*2*len(ngc_SUB_merM)+\n",
    "                                  np.repeat(['m','d','h'],2*len(ngc_USI_mer)).tolist(),\n",
    "                          'cor':np.repeat([ngc_BRA_mer,ngc_BRA_era],3).tolist() +\n",
    "                                (ngc_EST_merH.values.tolist() + ngc_EST_eraH.values.tolist())*2 + ngc_EST_merM.values.tolist() + ngc_EST_eraM.values.tolist() +\n",
    "                                 (ngc_SUB_merH.values.tolist() + ngc_SUB_eraH.values.tolist())*2 + ngc_SUB_merM.values.tolist() + ngc_SUB_eraM.values.tolist() +\n",
    "                                 (ngc_USI_mer.values.tolist() + ngc_USI_era.values.tolist())*3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save system sizes\n"
     ]
    }
   ],
   "source": [
    "# save wind speed correlations\n",
    "print('save system sizes')\n",
    "ngc.to_csv(results_path + '/number_grid_points.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-py37]",
   "language": "python",
   "name": "conda-env-.conda-py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
